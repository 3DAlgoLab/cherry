{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Cherry is reinforcement learning framework for researchers built on top of PyTorch. Unlike other reinforcement learning implementations, cherry doesn't try to provide a single interface to existing algorithms. Instead, it provides you with common tools to write your own algorithms. Drawing from the UNIX philosophy, each tool strives to be as independent from the rest of the framework as possible. So if you don't like a specific tool, you can still use parts of cherry without headaches. Installation For now, cherry is still in development. Clone the repo: git clone https://github.com/seba-1511/cherry cd cherry pip install -e . Upon our first public release, you'll be able to pip install cherry-rl Development Guidelines The master branch is always working, considered stable. The dev branch should always work and is ahead of master , considered cutting edge. To implement a new functionality: branch dev into your_name/functionality_name , implement your functionality, then pull request to dev . It will be periodically merged into master . Usage The following snippet demonstrates some of the tools offered by cherry. import cherry as ch # Wrapping environments env = ch.envs.Logger(env, interval=1000) # Prints rollouts statistics env = ch.envs.Normalized(env, normalize_state=True, normalize_reward=False) env = ch.envs.Torch(env) # Converts actions/states to tensors # Storing and retrieving experience replay = ch.ExperienceReplay() replay.add(old_state, action, reward, state, done, info = { 'log_prob': mass.log_prob(action), # Can add any variable/tensor to the transitions 'value': value }) replay.actions # Tensor of all stored actions replay.states # Tensor of all stored states replay.empty() # Removes all stored experience # Discounting and normalizing rewards rewards = ch.rewards.discount_rewards(GAMMA, replay.rewards, replay.dones) rewards = ch.utils.normalize(th.tensor(rewards)) # Sampling rollouts per episode or samples num_samples, num_episodes = ch.rollouts.collect(env, get_action, replay, num_episodes=10, # alternatively: num_samples=1000, ) Concrete examples are available in the examples/ folder. Documentation The documentation will be written as we begin to converge the core concepts of cherry. TODO Some functionalities that we might want to implement. parallelize environments and a way to handle it with ExperienceReplay , VisdomLogger as a dashboard to debug an implementation, example with reccurent net, minimal but complete documentation, a few extensive tutorials (Getting started with distributed A2C, Advanced usage (which?) with PPO, and another on debugging your algorithms). Acknowledgements Cherry draws inspiration from many reinforcement learning implementations, including OpenAI Baselines , John Schulman's implementations Ilya Kostrikov's implementations , Shangtong Zhang's implementations , RLLab , RLKit .","title":"Home"},{"location":"#installation","text":"For now, cherry is still in development. Clone the repo: git clone https://github.com/seba-1511/cherry cd cherry pip install -e . Upon our first public release, you'll be able to pip install cherry-rl","title":"Installation"},{"location":"#development-guidelines","text":"The master branch is always working, considered stable. The dev branch should always work and is ahead of master , considered cutting edge. To implement a new functionality: branch dev into your_name/functionality_name , implement your functionality, then pull request to dev . It will be periodically merged into master .","title":"Development Guidelines"},{"location":"#usage","text":"The following snippet demonstrates some of the tools offered by cherry. import cherry as ch # Wrapping environments env = ch.envs.Logger(env, interval=1000) # Prints rollouts statistics env = ch.envs.Normalized(env, normalize_state=True, normalize_reward=False) env = ch.envs.Torch(env) # Converts actions/states to tensors # Storing and retrieving experience replay = ch.ExperienceReplay() replay.add(old_state, action, reward, state, done, info = { 'log_prob': mass.log_prob(action), # Can add any variable/tensor to the transitions 'value': value }) replay.actions # Tensor of all stored actions replay.states # Tensor of all stored states replay.empty() # Removes all stored experience # Discounting and normalizing rewards rewards = ch.rewards.discount_rewards(GAMMA, replay.rewards, replay.dones) rewards = ch.utils.normalize(th.tensor(rewards)) # Sampling rollouts per episode or samples num_samples, num_episodes = ch.rollouts.collect(env, get_action, replay, num_episodes=10, # alternatively: num_samples=1000, ) Concrete examples are available in the examples/ folder.","title":"Usage"},{"location":"#documentation","text":"The documentation will be written as we begin to converge the core concepts of cherry.","title":"Documentation"},{"location":"#todo","text":"Some functionalities that we might want to implement. parallelize environments and a way to handle it with ExperienceReplay , VisdomLogger as a dashboard to debug an implementation, example with reccurent net, minimal but complete documentation, a few extensive tutorials (Getting started with distributed A2C, Advanced usage (which?) with PPO, and another on debugging your algorithms).","title":"TODO"},{"location":"#acknowledgements","text":"Cherry draws inspiration from many reinforcement learning implementations, including OpenAI Baselines , John Schulman's implementations Ilya Kostrikov's implementations , Shangtong Zhang's implementations , RLLab , RLKit .","title":"Acknowledgements"},{"location":"docs/cherry.algorithms/","text":"cherry.algorithms.a2c policy_loss policy_loss(log_probs, advantages) Advantage Actor-Critic policy loss. value_loss value_loss(values, rewards) Advantage Actor-Critic value loss. cherry.algorithms.ppo policy_loss policy_loss(new_log_probs, old_log_probs, advantages, clip=0.1) Clipped policy loss. Arguments: new_log_probs: (tensor) old_log_probs: (tensor) advantages: (tensor) clip: (tensor) value_loss value_loss(new_values, old_values, rewards, clip=0.1) Clipped value loss. Arguments: new_values: (tensor) old_values: (tensor) rewards: (tensor) clip: (tensor) cherry.algorithms.sac policy_loss policy_loss(log_probs, q_actions, alpha=1.0) Arguments: log_probs: the log density of the actions from the current policy on some states. q_actions: the Q-values for those same actions. alpha: the weight of the policy entropy. q_loss q_loss(q_old_pred, v_next, rewards, dones, gamma) Arguments: q_old_pred: Q values on an existing transition. v_next: V values for the resulting state. rewards: observed rewards during the transition. dones: which states were terminal. gamma: discount factor. v_loss v_loss(v_pred, log_probs, q_values, alpha=1.0) Arguments: v_pred: the V values of states from a batch. log_probs: the log density of actions from the current policy on those states. q_values: the Q values of the those actions on those states. alpha: the weight of the policy entropy.","title":"cherry.algorithms"},{"location":"docs/cherry.algorithms/#cherryalgorithmsa2c","text":"","title":"cherry.algorithms.a2c"},{"location":"docs/cherry.algorithms/#policy_loss","text":"policy_loss(log_probs, advantages) Advantage Actor-Critic policy loss.","title":"policy_loss"},{"location":"docs/cherry.algorithms/#value_loss","text":"value_loss(values, rewards) Advantage Actor-Critic value loss.","title":"value_loss"},{"location":"docs/cherry.algorithms/#cherryalgorithmsppo","text":"","title":"cherry.algorithms.ppo"},{"location":"docs/cherry.algorithms/#policy_loss_1","text":"policy_loss(new_log_probs, old_log_probs, advantages, clip=0.1) Clipped policy loss. Arguments: new_log_probs: (tensor) old_log_probs: (tensor) advantages: (tensor) clip: (tensor)","title":"policy_loss"},{"location":"docs/cherry.algorithms/#value_loss_1","text":"value_loss(new_values, old_values, rewards, clip=0.1) Clipped value loss. Arguments: new_values: (tensor) old_values: (tensor) rewards: (tensor) clip: (tensor)","title":"value_loss"},{"location":"docs/cherry.algorithms/#cherryalgorithmssac","text":"","title":"cherry.algorithms.sac"},{"location":"docs/cherry.algorithms/#policy_loss_2","text":"policy_loss(log_probs, q_actions, alpha=1.0) Arguments: log_probs: the log density of the actions from the current policy on some states. q_actions: the Q-values for those same actions. alpha: the weight of the policy entropy.","title":"policy_loss"},{"location":"docs/cherry.algorithms/#q_loss","text":"q_loss(q_old_pred, v_next, rewards, dones, gamma) Arguments: q_old_pred: Q values on an existing transition. v_next: V values for the resulting state. rewards: observed rewards during the transition. dones: which states were terminal. gamma: discount factor.","title":"q_loss"},{"location":"docs/cherry.algorithms/#v_loss","text":"v_loss(v_pred, log_probs, q_values, alpha=1.0) Arguments: v_pred: the V values of states from a batch. log_probs: the log density of actions from the current policy on those states. q_values: the Q values of the those actions on those states. alpha: the weight of the policy entropy.","title":"v_loss"},{"location":"docs/cherry.distributions/","text":"cherry.distributions TanhNormal TanhNormal(self, normal_mean, normal_std) Adapted from Vitchyr Pong's RLkit: https://github.com/vitchyr/rlkit/blob/master/rlkit/torch/distributions.py","title":"cherry.distributions"},{"location":"docs/cherry.distributions/#cherrydistributions","text":"","title":"cherry.distributions"},{"location":"docs/cherry.distributions/#tanhnormal","text":"TanhNormal(self, normal_mean, normal_std) Adapted from Vitchyr Pong's RLkit: https://github.com/vitchyr/rlkit/blob/master/rlkit/torch/distributions.py","title":"TanhNormal"},{"location":"docs/cherry.envs/","text":"Wrapper Wrapper(self, env) This class allows to chain Environment Wrappers while still being able to access the properties of wrapped wrappers. Example: env = gym.make('MyEnv-v0') env = envs.Logger(env) env = envs.Runner(env) env.log('asdf', 23) # Uses log() method from envs.Logger. Runner Runner(self, env) Runner wrapper. run Runner.run(self, get_action, steps=None, episodes=None, render=False) Runner wrapper's run method. Logger Logger(self, env, interval=1000, episode_interval=10) Tracks and prints some common statistics about the environment. Torch Torch(self, env) This wrapper converts * actions from Tensors to numpy, * states from lists/numpy to Tensors. Example: action = Categorical(Tensor([1, 2, 3])).sample() env.step(action) Normalized Normalized(self, env, scale_reward=1.0, normalize_state=True, normalize_reward=False, state_alpha=0.001, reward_alpha=0.001) Normalizing wrapper for actions, states, and rewards. Atari Atari(self, env, grayscale=True, skip=4, stack=4, warp=84) Process Atari frames similarly to what DeepMind does. Skip 3 out of 4 frames, Stack the last 4 frames, Warp state to a 84x84 image (optionally grayscale). Note: The reward clipping is available in cherry.envs.ClipRewards. step Atari.step(self, *args, **kwargs) Sum rewards, max over observations. OpenAIAtari OpenAIAtari(self, env) ClipReward ClipReward(self, env) reward ClipReward.reward(self, reward) Bin reward to {+1, 0, -1} by its sign. AddTimestep AddTimestep(self, env=None) Adds a timestep information to the state input. Modified from Ilya Kostrikov's implementation: https://github.com/ikostrikov/pytorch-a2c-ppo-acktr/ Monitor Monitor(self, env, directory, *args, **kwargs) Sugar coating on top of Gym's Monitor. OpenAINormalize OpenAINormalize(self, env, state=True, ret=True, clipob=10.0, cliprew=10.0, gamma=0.99, eps=1e-08) StateLambda StateLambda(self, env, fn) ActionSpaceScaler ActionSpaceScaler(self, env, clip=1.0) Scales the action space to be in the range (-clip, clip). Adapted from Vitchyr Pong's RLkit: https://github.com/vitchyr/rlkit/blob/master/rlkit/envs/wrappers.py#L41 VisdomLogger VisdomLogger(self, env, interval=1000, episode_interval=100, name=None) Enables logging and debug values to Visdom. Arguments env: The environment to wrap. interval: (int) Update frequency for episodes.","title":"cherry.envs"},{"location":"docs/cherry.envs/#wrapper","text":"Wrapper(self, env) This class allows to chain Environment Wrappers while still being able to access the properties of wrapped wrappers. Example: env = gym.make('MyEnv-v0') env = envs.Logger(env) env = envs.Runner(env) env.log('asdf', 23) # Uses log() method from envs.Logger.","title":"Wrapper"},{"location":"docs/cherry.envs/#runner","text":"Runner(self, env) Runner wrapper.","title":"Runner"},{"location":"docs/cherry.envs/#run","text":"Runner.run(self, get_action, steps=None, episodes=None, render=False) Runner wrapper's run method.","title":"run"},{"location":"docs/cherry.envs/#logger","text":"Logger(self, env, interval=1000, episode_interval=10) Tracks and prints some common statistics about the environment.","title":"Logger"},{"location":"docs/cherry.envs/#torch","text":"Torch(self, env) This wrapper converts * actions from Tensors to numpy, * states from lists/numpy to Tensors. Example: action = Categorical(Tensor([1, 2, 3])).sample() env.step(action)","title":"Torch"},{"location":"docs/cherry.envs/#normalized","text":"Normalized(self, env, scale_reward=1.0, normalize_state=True, normalize_reward=False, state_alpha=0.001, reward_alpha=0.001) Normalizing wrapper for actions, states, and rewards.","title":"Normalized"},{"location":"docs/cherry.envs/#atari","text":"Atari(self, env, grayscale=True, skip=4, stack=4, warp=84) Process Atari frames similarly to what DeepMind does. Skip 3 out of 4 frames, Stack the last 4 frames, Warp state to a 84x84 image (optionally grayscale). Note: The reward clipping is available in cherry.envs.ClipRewards.","title":"Atari"},{"location":"docs/cherry.envs/#step","text":"Atari.step(self, *args, **kwargs) Sum rewards, max over observations.","title":"step"},{"location":"docs/cherry.envs/#openaiatari","text":"OpenAIAtari(self, env)","title":"OpenAIAtari"},{"location":"docs/cherry.envs/#clipreward","text":"ClipReward(self, env)","title":"ClipReward"},{"location":"docs/cherry.envs/#reward","text":"ClipReward.reward(self, reward) Bin reward to {+1, 0, -1} by its sign.","title":"reward"},{"location":"docs/cherry.envs/#addtimestep","text":"AddTimestep(self, env=None) Adds a timestep information to the state input. Modified from Ilya Kostrikov's implementation: https://github.com/ikostrikov/pytorch-a2c-ppo-acktr/","title":"AddTimestep"},{"location":"docs/cherry.envs/#monitor","text":"Monitor(self, env, directory, *args, **kwargs) Sugar coating on top of Gym's Monitor.","title":"Monitor"},{"location":"docs/cherry.envs/#openainormalize","text":"OpenAINormalize(self, env, state=True, ret=True, clipob=10.0, cliprew=10.0, gamma=0.99, eps=1e-08)","title":"OpenAINormalize"},{"location":"docs/cherry.envs/#statelambda","text":"StateLambda(self, env, fn)","title":"StateLambda"},{"location":"docs/cherry.envs/#actionspacescaler","text":"ActionSpaceScaler(self, env, clip=1.0) Scales the action space to be in the range (-clip, clip). Adapted from Vitchyr Pong's RLkit: https://github.com/vitchyr/rlkit/blob/master/rlkit/envs/wrappers.py#L41","title":"ActionSpaceScaler"},{"location":"docs/cherry.envs/#visdomlogger","text":"VisdomLogger(self, env, interval=1000, episode_interval=100, name=None) Enables logging and debug values to Visdom. Arguments env: The environment to wrap. interval: (int) Update frequency for episodes.","title":"VisdomLogger"},{"location":"docs/cherry.experience-replay/","text":"Transition Transition(self, **kwargs) Description Represents a (s, a, r, s', d) tuple. Arguments None Example for transition in replay: print(transition.state) ExperienceReplay ExperienceReplay(self, states=None, actions=None, rewards=None, next_states=None, dones=None, infos=None) [Source] Description Experience replay buffer to store, retrieve, and sample past transitions. ExperienceReplay behaves like a list of transitions, . It also support accessing specific properties, such as states, actions, rewards, next_states, and informations. The first four are returned as tensors, while infos is returned as a list of dicts. The properties of infos can be accessed directly by appending an s to their dictionary key -- see Examples below. In this case, if the values of the infos are tensors, they will be returned as a concatenated Tensor. Otherwise, they default to a list of values. Arguments states (Tensor, optional , default=None) - Tensor of states. actions (Tensor, optional , default=None) - Tensor of actions. rewards (Tensor, optional , default=None) - Tensor of rewards. next_states (Tensor, optional , default=None) - Tensor of next_states. dones (Tensor, optional , default=None) - Tensor of dones. infos (list, optional , default=None) - List of infos. References Lin, Long-Ji. 1992. \u201cSelf-Improving Reactive Agents Based on Reinforcement Learning, Planning and Teaching.\u201d Machine Learning 8 (3): 293\u2013321. Example replay = ch.ExperienceReplay() # Instanciate a new replay replay.add(state, # Add experience to the replay action, reward, next_state, done, info={ 'density': action_density, 'log_prob': action_density.log_prob(action), }) replay.state # Tensor of states replay.actions # Tensor of actions replay.densitys # list of action_density replay.log_probs # Tensor of log_probabilities new_replay = replay[-10:] # Last 10 transitions in new_replay #Sample some previous experience batch = replay.sample(32, contiguous=True) sample ExperienceReplay.sample(self, size=1, contiguous=False, episodes=False) Samples from the Experience replay. Arguments size (int, optional , default=1) - The number of samples. contiguous (bool, optional , default=False) - Whether to sample contiguous transitions. episodes (bool, optional , default=False) - Sample full episodes, instead of transitions. Return ExperienceReplay() containing the sampled transitions.","title":"cherry.ExperienceReplay"},{"location":"docs/cherry.experience-replay/#transition","text":"Transition(self, **kwargs) Description Represents a (s, a, r, s', d) tuple. Arguments None Example for transition in replay: print(transition.state)","title":"Transition"},{"location":"docs/cherry.experience-replay/#experiencereplay","text":"ExperienceReplay(self, states=None, actions=None, rewards=None, next_states=None, dones=None, infos=None) [Source] Description Experience replay buffer to store, retrieve, and sample past transitions. ExperienceReplay behaves like a list of transitions, . It also support accessing specific properties, such as states, actions, rewards, next_states, and informations. The first four are returned as tensors, while infos is returned as a list of dicts. The properties of infos can be accessed directly by appending an s to their dictionary key -- see Examples below. In this case, if the values of the infos are tensors, they will be returned as a concatenated Tensor. Otherwise, they default to a list of values. Arguments states (Tensor, optional , default=None) - Tensor of states. actions (Tensor, optional , default=None) - Tensor of actions. rewards (Tensor, optional , default=None) - Tensor of rewards. next_states (Tensor, optional , default=None) - Tensor of next_states. dones (Tensor, optional , default=None) - Tensor of dones. infos (list, optional , default=None) - List of infos. References Lin, Long-Ji. 1992. \u201cSelf-Improving Reactive Agents Based on Reinforcement Learning, Planning and Teaching.\u201d Machine Learning 8 (3): 293\u2013321. Example replay = ch.ExperienceReplay() # Instanciate a new replay replay.add(state, # Add experience to the replay action, reward, next_state, done, info={ 'density': action_density, 'log_prob': action_density.log_prob(action), }) replay.state # Tensor of states replay.actions # Tensor of actions replay.densitys # list of action_density replay.log_probs # Tensor of log_probabilities new_replay = replay[-10:] # Last 10 transitions in new_replay #Sample some previous experience batch = replay.sample(32, contiguous=True)","title":"ExperienceReplay"},{"location":"docs/cherry.experience-replay/#sample","text":"ExperienceReplay.sample(self, size=1, contiguous=False, episodes=False) Samples from the Experience replay. Arguments size (int, optional , default=1) - The number of samples. contiguous (bool, optional , default=False) - Whether to sample contiguous transitions. episodes (bool, optional , default=False) - Sample full episodes, instead of transitions. Return ExperienceReplay() containing the sampled transitions.","title":"sample"},{"location":"docs/cherry.nn.init/","text":"cherry.nn.init","title":"cherry.nn.init"},{"location":"docs/cherry.nn.init/#cherrynninit","text":"","title":"cherry.nn.init"},{"location":"docs/cherry.nn/","text":"cherry.nn","title":"cherry.nn"},{"location":"docs/cherry.nn/#cherrynn","text":"","title":"cherry.nn"},{"location":"docs/cherry.optim/","text":"cherry.optim Distributed Distributed(self, params=<required parameter>, opt=<required parameter>) Synchronizes optimizers across distributed replicas.","title":"cherry.optim"},{"location":"docs/cherry.optim/#cherryoptim","text":"","title":"cherry.optim"},{"location":"docs/cherry.optim/#distributed","text":"Distributed(self, params=<required parameter>, opt=<required parameter>) Synchronizes optimizers across distributed replicas.","title":"Distributed"},{"location":"docs/cherry.rewards/","text":"cherry.rewards discount_rewards discount_rewards(gamma, rewards, dones, bootstrap=0.0) discount_rewards discount_rewards(gamma, rewards, dones, bootstrap=0.0) generalized_advantage_estimate generalized_advantage_estimate(gamma, tau, rewards, dones, values, next_value) generalized_advantage_estimate generalized_advantage_estimate(gamma, tau, rewards, dones, values, next_value)","title":"cherry.rewards"},{"location":"docs/cherry.rewards/#cherryrewards","text":"","title":"cherry.rewards"},{"location":"docs/cherry.rewards/#discount_rewards","text":"discount_rewards(gamma, rewards, dones, bootstrap=0.0)","title":"discount_rewards"},{"location":"docs/cherry.rewards/#discount_rewards_1","text":"discount_rewards(gamma, rewards, dones, bootstrap=0.0)","title":"discount_rewards"},{"location":"docs/cherry.rewards/#generalized_advantage_estimate","text":"generalized_advantage_estimate(gamma, tau, rewards, dones, values, next_value)","title":"generalized_advantage_estimate"},{"location":"docs/cherry.rewards/#generalized_advantage_estimate_1","text":"generalized_advantage_estimate(gamma, tau, rewards, dones, values, next_value)","title":"generalized_advantage_estimate"},{"location":"docs/cherry.utils/","text":"cherry.utils normalize normalize(tensor, epsilon=2.220446049250313e-16) polyak_average polyak_average(source, target, alpha) flatten_state flatten_state(space, state) get_space_dimension get_space_dimension(space)","title":"cherry.utils"},{"location":"docs/cherry.utils/#cherryutils","text":"","title":"cherry.utils"},{"location":"docs/cherry.utils/#normalize","text":"normalize(tensor, epsilon=2.220446049250313e-16)","title":"normalize"},{"location":"docs/cherry.utils/#polyak_average","text":"polyak_average(source, target, alpha)","title":"polyak_average"},{"location":"docs/cherry.utils/#flatten_state","text":"flatten_state(space, state)","title":"flatten_state"},{"location":"docs/cherry.utils/#get_space_dimension","text":"get_space_dimension(space)","title":"get_space_dimension"},{"location":"tutorials/continuous_ppo/","text":"","title":"Debugging Reinforcement Learning"},{"location":"tutorials/distributed_a2c/","text":"Tutorial on Distributed A2C","title":"Distributed Policy Gradients with A2C"},{"location":"tutorials/distributed_a2c/#tutorial-on-distributed-a2c","text":"","title":"Tutorial on Distributed A2C"},{"location":"tutorials/getting_started/","text":"TOC Install Explain philosophy Overview of tools Use for a very simple example (e.g. reinforce)","title":"Getting Started with Cherry"},{"location":"tutorials/getting_started/#toc","text":"Install Explain philosophy Overview of tools Use for a very simple example (e.g. reinforce)","title":"TOC"}]}