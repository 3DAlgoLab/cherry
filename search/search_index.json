{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Cherry is a reinforcement learning framework for researchers built on top of PyTorch. Unlike other reinforcement learning implementations, cherry doesn't try to provide a single interface to existing algorithms. Instead, it provides you with common tools to write your own algorithms. Drawing from the UNIX philosophy, each tool strives to be as independent from the rest of the framework as possible. So if you don't like a specific tool, you can still use parts of cherry without headaches. Features Pythonic and modular interface \u00e0 la Pytorch. Support for tabular (!) and function approximation algorithms. Various OpenAI Gym environment wrappers. Helper functions for popular algorithms. (e.g. A2C, DDPG, TRPO, PPO, SAC) Logging, visualization, and debugging tools. Painless and efficient distributed training on CPUs and GPUs. Unit, integration, and regression tested, continuously integrated. Installation For now, cherry is still in development. Clone the repo: git clone https://github.com/seba-1511/cherry cd cherry pip install -e . Upon our first public release, you'll be able to pip install cherry-rl Development Guidelines The master branch is always working, considered stable. The dev branch should always work and is ahead of master , considered cutting edge. To implement a new functionality: branch dev into your_name/functionality_name , implement your functionality, then pull request to dev . It will be periodically merged into master . Usage The following snippet demonstrates some of the tools offered by cherry. import cherry as ch # Wrapping environments env = ch.envs.Logger(env, interval=1000) # Prints rollouts statistics env = ch.envs.Normalized(env, normalize_state=True, normalize_reward=False) env = ch.envs.Torch(env) # Converts actions/states to tensors # Storing and retrieving experience replay = ch.ExperienceReplay() replay.append(old_state, action, reward, state, done, info = { 'log_prob': mass.log_prob(action), # Can add any variable/tensor to the transitions 'value': value }) replay.actions # Tensor of all stored actions replay.states # Tensor of all stored states replay.empty() # Removes all stored experience # Discounting and normalizing rewards rewards = ch.rewards.discount(GAMMA, replay.rewards, replay.dones) rewards = ch.utils.normalize(rewards) # Sampling rollouts per episode or samples num_samples, num_episodes = ch.rollouts.collect(env, get_action, replay, num_episodes=10, # alternatively: num_samples=1000, ) Concrete examples are available in the examples/ folder. Documentation The documentation will be written as we begin to converge the core concepts of cherry. TODO Some functionalities that we might want to implement. parallelize environments and a way to handle it with ExperienceReplay , VisdomLogger as a dashboard to debug an implementation, example with reccurent net, minimal but complete documentation, GPU implementations. Acknowledgements Cherry draws inspiration from many reinforcement learning implementations, including OpenAI Baselines , John Schulman's implementations Ilya Kostrikov's implementations , Shangtong Zhang's implementations , Dave Abel's implementations , Vitchyr Pong's implementations , RLLab .","title":"Home"},{"location":"#installation","text":"For now, cherry is still in development. Clone the repo: git clone https://github.com/seba-1511/cherry cd cherry pip install -e . Upon our first public release, you'll be able to pip install cherry-rl","title":"Installation"},{"location":"#development-guidelines","text":"The master branch is always working, considered stable. The dev branch should always work and is ahead of master , considered cutting edge. To implement a new functionality: branch dev into your_name/functionality_name , implement your functionality, then pull request to dev . It will be periodically merged into master .","title":"Development Guidelines"},{"location":"#usage","text":"The following snippet demonstrates some of the tools offered by cherry. import cherry as ch # Wrapping environments env = ch.envs.Logger(env, interval=1000) # Prints rollouts statistics env = ch.envs.Normalized(env, normalize_state=True, normalize_reward=False) env = ch.envs.Torch(env) # Converts actions/states to tensors # Storing and retrieving experience replay = ch.ExperienceReplay() replay.append(old_state, action, reward, state, done, info = { 'log_prob': mass.log_prob(action), # Can add any variable/tensor to the transitions 'value': value }) replay.actions # Tensor of all stored actions replay.states # Tensor of all stored states replay.empty() # Removes all stored experience # Discounting and normalizing rewards rewards = ch.rewards.discount(GAMMA, replay.rewards, replay.dones) rewards = ch.utils.normalize(rewards) # Sampling rollouts per episode or samples num_samples, num_episodes = ch.rollouts.collect(env, get_action, replay, num_episodes=10, # alternatively: num_samples=1000, ) Concrete examples are available in the examples/ folder.","title":"Usage"},{"location":"#documentation","text":"The documentation will be written as we begin to converge the core concepts of cherry.","title":"Documentation"},{"location":"#todo","text":"Some functionalities that we might want to implement. parallelize environments and a way to handle it with ExperienceReplay , VisdomLogger as a dashboard to debug an implementation, example with reccurent net, minimal but complete documentation, GPU implementations.","title":"TODO"},{"location":"#acknowledgements","text":"Cherry draws inspiration from many reinforcement learning implementations, including OpenAI Baselines , John Schulman's implementations Ilya Kostrikov's implementations , Shangtong Zhang's implementations , Dave Abel's implementations , Vitchyr Pong's implementations , RLLab .","title":"Acknowledgements"},{"location":"docs/cherry.algorithms/","text":"cherry.algorithms.a2c Helper functions for A2C. policy_loss policy_loss(log_probs, advantages) [Source] Description Advantage Actor-Critic policy loss. References Arguments Returns Example advantages = replay.advantages log_probs = replay.log_probs loss = a2c.policy_loss(log_probs, advantages) state_value_loss state_value_loss(values, rewards) Advantage Actor-Critic value loss. cherry.algorithms.ppo policy_loss policy_loss(new_log_probs, old_log_probs, advantages, clip=0.1) Clipped policy loss. Arguments: new_log_probs: (tensor) old_log_probs: (tensor) advantages: (tensor) clip: (tensor) state_value_loss state_value_loss(new_values, old_values, rewards, clip=0.1) Clipped value loss. Arguments: new_values: (tensor) old_values: (tensor) rewards: (tensor) clip: (tensor) cherry.algorithms.sac policy_loss policy_loss(log_probs, q_actions, alpha=1.0) Arguments: log_probs: the log density of the actions from the current policy on some states. q_actions: the Q-values for those same actions. alpha: the weight of the policy entropy. action_value_loss action_value_loss(q_old_pred, v_next, rewards, dones, gamma) Arguments: q_old_pred: Q values on an existing transition. v_next: V values for the resulting state. rewards: observed rewards during the transition. dones: which states were terminal. gamma: discount factor. state_value_loss state_value_loss(v_pred, log_probs, q_values, alpha=1.0) Arguments: v_pred: the V values of states from a batch. log_probs: the log density of actions from the current policy on those states. q_values: the Q values of the those actions on those states. alpha: the weight of the policy entropy.","title":"cherry.algorithms"},{"location":"docs/cherry.algorithms/#cherryalgorithmsa2c","text":"Helper functions for A2C.","title":"cherry.algorithms.a2c"},{"location":"docs/cherry.algorithms/#policy_loss","text":"policy_loss(log_probs, advantages) [Source] Description Advantage Actor-Critic policy loss. References Arguments Returns Example advantages = replay.advantages log_probs = replay.log_probs loss = a2c.policy_loss(log_probs, advantages)","title":"policy_loss"},{"location":"docs/cherry.algorithms/#state_value_loss","text":"state_value_loss(values, rewards) Advantage Actor-Critic value loss.","title":"state_value_loss"},{"location":"docs/cherry.algorithms/#cherryalgorithmsppo","text":"","title":"cherry.algorithms.ppo"},{"location":"docs/cherry.algorithms/#policy_loss_1","text":"policy_loss(new_log_probs, old_log_probs, advantages, clip=0.1) Clipped policy loss. Arguments: new_log_probs: (tensor) old_log_probs: (tensor) advantages: (tensor) clip: (tensor)","title":"policy_loss"},{"location":"docs/cherry.algorithms/#state_value_loss_1","text":"state_value_loss(new_values, old_values, rewards, clip=0.1) Clipped value loss. Arguments: new_values: (tensor) old_values: (tensor) rewards: (tensor) clip: (tensor)","title":"state_value_loss"},{"location":"docs/cherry.algorithms/#cherryalgorithmssac","text":"","title":"cherry.algorithms.sac"},{"location":"docs/cherry.algorithms/#policy_loss_2","text":"policy_loss(log_probs, q_actions, alpha=1.0) Arguments: log_probs: the log density of the actions from the current policy on some states. q_actions: the Q-values for those same actions. alpha: the weight of the policy entropy.","title":"policy_loss"},{"location":"docs/cherry.algorithms/#action_value_loss","text":"action_value_loss(q_old_pred, v_next, rewards, dones, gamma) Arguments: q_old_pred: Q values on an existing transition. v_next: V values for the resulting state. rewards: observed rewards during the transition. dones: which states were terminal. gamma: discount factor.","title":"action_value_loss"},{"location":"docs/cherry.algorithms/#state_value_loss_2","text":"state_value_loss(v_pred, log_probs, q_values, alpha=1.0) Arguments: v_pred: the V values of states from a batch. log_probs: the log density of actions from the current policy on those states. q_values: the Q values of the those actions on those states. alpha: the weight of the policy entropy.","title":"state_value_loss"},{"location":"docs/cherry.distributions/","text":"cherry.distributions Reparameterization Reparameterization(self, density) Reparameterized distribution. ActionDistribution ActionDistribution(self, env, logstd=None, use_probs=False, reparam=False) A helper module to automatically choose the proper policy distribution, based on the environment action_space. TanhNormal TanhNormal(self, normal_mean, normal_std) Adapted from Vitchyr Pong's RLkit: https://github.com/vitchyr/rlkit/blob/master/rlkit/torch/distributions.py","title":"cherry.distributions"},{"location":"docs/cherry.distributions/#cherrydistributions","text":"","title":"cherry.distributions"},{"location":"docs/cherry.distributions/#reparameterization","text":"Reparameterization(self, density) Reparameterized distribution.","title":"Reparameterization"},{"location":"docs/cherry.distributions/#actiondistribution","text":"ActionDistribution(self, env, logstd=None, use_probs=False, reparam=False) A helper module to automatically choose the proper policy distribution, based on the environment action_space.","title":"ActionDistribution"},{"location":"docs/cherry.distributions/#tanhnormal","text":"TanhNormal(self, normal_mean, normal_std) Adapted from Vitchyr Pong's RLkit: https://github.com/vitchyr/rlkit/blob/master/rlkit/torch/distributions.py","title":"TanhNormal"},{"location":"docs/cherry.envs/","text":"cherry.envs.utils get_space_dimension get_space_dimension(space) Returns the number of elements of a space sample, when unrolled. Wrapper Wrapper(self, env) This class allows to chain Environment Wrappers while still being able to access the properties of wrapped wrappers. Example: env = gym.make('MyEnv-v0') env = envs.Logger(env) env = envs.Runner(env) env.log('asdf', 23) # Uses log() method from envs.Logger. Runner Runner(self, env) Runner wrapper. run Runner.run(self, get_action, steps=None, episodes=None, render=False) Runner wrapper's run method. Logger Logger(self, env, interval=1000, episode_interval=10) Tracks and prints some common statistics about the environment. Recorder Recorder(self, env, directory='./videos/', format='gif', suffix=None) [Source] Description Wrapper to record episodes from a rollout. Supports GIF and MP4 encoding. Arguments env (Environment) - Environment to record. directory (str, optional , default='./videos/') - Relative path to where videos will be saved. format (str, optional , default='gif') - Format of the videos. Choose in ['gif', 'mp4'], defaults to gif. If it's text environment, the format will be json. suffix (str, optional , default=None): A unique id used as part of the suffix for the file. By default, uses os.getpid(). Credit Adapted from OpenAI Gym's Monitor wrapper. Example env = gym.make('CartPole-v0') env = envs.Recorder(record_env, './videos/', format='gif') env = envs.Runner(env) env.run(get_action, episodes=3, render=True) close Recorder.close(self) Flush all monitor data to disk and close any open rending windows VisdomLogger VisdomLogger(self, env, interval=1000, episode_interval=100, name=None) Enables logging and debug values to Visdom. Arguments env: The environment to wrap. interval: (int) Update frequency for episodes. Torch Torch(self, env) This wrapper converts * actions from Tensors to numpy, * states from lists/numpy to Tensors. Example: action = Categorical(Tensor([1, 2, 3])).sample() env.step(action) Normalizer Normalizer(self, env, states=True, rewards=True, clip_states=10.0, clip_rewards=10.0, gamma=0.99, eps=1e-08) [Source] Description Normalizes the states and rewards with a running average. Arguments env (Environment) - Environment to normalize. states (bool, optional , default=True) - Whether to normalize the states. rewards (bool, optional , default=True) - Whether to normalize the rewards. clip_states (bool, optional , default=10.0) - Clip each state dimension between [-clip_states, clip_states]. clip_rewards (float, optional , default=10.0) - Clip rewards between [-clip_rewards, clip_rewards]. gamma (float, optional , default=0.99) - Discount factor for rewards running averages. eps (float, optional , default=1e-8) - Numerical stability. Credit Adapted from OpenAI's baselines implementation. Example env = gym.make('CartPole-v0') env = cherry.envs.Normalizer(env, states=True, rewards=False) RewardClipper RewardClipper(self, env) reward RewardClipper.reward(self, reward) Bin reward to {+1, 0, -1} by its sign. Monitor Monitor(self, env, directory, *args, **kwargs) Sugar coating on top of Gym's Monitor. OpenAIAtari OpenAIAtari(self, env) AddTimestep AddTimestep(self, env=None) Adds a timestep information to the state input. Modified from Ilya Kostrikov's implementation: https://github.com/ikostrikov/pytorch-a2c-ppo-acktr/ StateLambda StateLambda(self, env, fn) ActionSpaceScaler ActionSpaceScaler(self, env, clip=1.0) Scales the action space to be in the range (-clip, clip). Adapted from Vitchyr Pong's RLkit: https://github.com/vitchyr/rlkit/blob/master/rlkit/envs/wrappers.py#L41","title":"cherry.envs"},{"location":"docs/cherry.envs/#cherryenvsutils","text":"","title":"cherry.envs.utils"},{"location":"docs/cherry.envs/#get_space_dimension","text":"get_space_dimension(space) Returns the number of elements of a space sample, when unrolled.","title":"get_space_dimension"},{"location":"docs/cherry.envs/#wrapper","text":"Wrapper(self, env) This class allows to chain Environment Wrappers while still being able to access the properties of wrapped wrappers. Example: env = gym.make('MyEnv-v0') env = envs.Logger(env) env = envs.Runner(env) env.log('asdf', 23) # Uses log() method from envs.Logger.","title":"Wrapper"},{"location":"docs/cherry.envs/#runner","text":"Runner(self, env) Runner wrapper.","title":"Runner"},{"location":"docs/cherry.envs/#run","text":"Runner.run(self, get_action, steps=None, episodes=None, render=False) Runner wrapper's run method.","title":"run"},{"location":"docs/cherry.envs/#logger","text":"Logger(self, env, interval=1000, episode_interval=10) Tracks and prints some common statistics about the environment.","title":"Logger"},{"location":"docs/cherry.envs/#recorder","text":"Recorder(self, env, directory='./videos/', format='gif', suffix=None) [Source] Description Wrapper to record episodes from a rollout. Supports GIF and MP4 encoding. Arguments env (Environment) - Environment to record. directory (str, optional , default='./videos/') - Relative path to where videos will be saved. format (str, optional , default='gif') - Format of the videos. Choose in ['gif', 'mp4'], defaults to gif. If it's text environment, the format will be json. suffix (str, optional , default=None): A unique id used as part of the suffix for the file. By default, uses os.getpid(). Credit Adapted from OpenAI Gym's Monitor wrapper. Example env = gym.make('CartPole-v0') env = envs.Recorder(record_env, './videos/', format='gif') env = envs.Runner(env) env.run(get_action, episodes=3, render=True)","title":"Recorder"},{"location":"docs/cherry.envs/#close","text":"Recorder.close(self) Flush all monitor data to disk and close any open rending windows","title":"close"},{"location":"docs/cherry.envs/#visdomlogger","text":"VisdomLogger(self, env, interval=1000, episode_interval=100, name=None) Enables logging and debug values to Visdom. Arguments env: The environment to wrap. interval: (int) Update frequency for episodes.","title":"VisdomLogger"},{"location":"docs/cherry.envs/#torch","text":"Torch(self, env) This wrapper converts * actions from Tensors to numpy, * states from lists/numpy to Tensors. Example: action = Categorical(Tensor([1, 2, 3])).sample() env.step(action)","title":"Torch"},{"location":"docs/cherry.envs/#normalizer","text":"Normalizer(self, env, states=True, rewards=True, clip_states=10.0, clip_rewards=10.0, gamma=0.99, eps=1e-08) [Source] Description Normalizes the states and rewards with a running average. Arguments env (Environment) - Environment to normalize. states (bool, optional , default=True) - Whether to normalize the states. rewards (bool, optional , default=True) - Whether to normalize the rewards. clip_states (bool, optional , default=10.0) - Clip each state dimension between [-clip_states, clip_states]. clip_rewards (float, optional , default=10.0) - Clip rewards between [-clip_rewards, clip_rewards]. gamma (float, optional , default=0.99) - Discount factor for rewards running averages. eps (float, optional , default=1e-8) - Numerical stability. Credit Adapted from OpenAI's baselines implementation. Example env = gym.make('CartPole-v0') env = cherry.envs.Normalizer(env, states=True, rewards=False)","title":"Normalizer"},{"location":"docs/cherry.envs/#rewardclipper","text":"RewardClipper(self, env)","title":"RewardClipper"},{"location":"docs/cherry.envs/#reward","text":"RewardClipper.reward(self, reward) Bin reward to {+1, 0, -1} by its sign.","title":"reward"},{"location":"docs/cherry.envs/#monitor","text":"Monitor(self, env, directory, *args, **kwargs) Sugar coating on top of Gym's Monitor.","title":"Monitor"},{"location":"docs/cherry.envs/#openaiatari","text":"OpenAIAtari(self, env)","title":"OpenAIAtari"},{"location":"docs/cherry.envs/#addtimestep","text":"AddTimestep(self, env=None) Adds a timestep information to the state input. Modified from Ilya Kostrikov's implementation: https://github.com/ikostrikov/pytorch-a2c-ppo-acktr/","title":"AddTimestep"},{"location":"docs/cherry.envs/#statelambda","text":"StateLambda(self, env, fn)","title":"StateLambda"},{"location":"docs/cherry.envs/#actionspacescaler","text":"ActionSpaceScaler(self, env, clip=1.0) Scales the action space to be in the range (-clip, clip). Adapted from Vitchyr Pong's RLkit: https://github.com/vitchyr/rlkit/blob/master/rlkit/envs/wrappers.py#L41","title":"ActionSpaceScaler"},{"location":"docs/cherry.experience-replay/","text":"Transition Transition(self, state, action, reward, next_state, done, info=None) Description Represents a (s, a, r, s', d) tuple. Arguments state (tensor) - Originating state. action (tensor) - Executed action. reward (tensor) - Observed reward. next_state (tensor) - Resulting state. done (tensor) - Is next_state a terminal (absorbing) state ? info (dict, optional , default=None) - Additional information on the transition. Example for transition in replay: print(transition.state) ExperienceReplay ExperienceReplay(self, states=None, actions=None, rewards=None, next_states=None, dones=None, infos=None) [Source] Description Experience replay buffer to store, retrieve, and sample past transitions. ExperienceReplay behaves like a list of transitions, . It also support accessing specific properties, such as states, actions, rewards, next_states, and informations. The first four are returned as tensors, while infos is returned as a list of dicts. The properties of infos can be accessed directly by appending an s to their dictionary key -- see Examples below. In this case, if the values of the infos are tensors, they will be returned as a concatenated Tensor. Otherwise, they default to a list of values. Arguments states (Tensor, optional , default=None) - Tensor of states. actions (Tensor, optional , default=None) - Tensor of actions. rewards (Tensor, optional , default=None) - Tensor of rewards. next_states (Tensor, optional , default=None) - Tensor of next_states. dones (Tensor, optional , default=None) - Tensor of dones. infos (list, optional , default=None) - List of infos. References Lin, Long-Ji. 1992. \u201cSelf-Improving Reactive Agents Based on Reinforcement Learning, Planning and Teaching.\u201d Machine Learning 8 (3): 293\u2013321. Example replay = ch.ExperienceReplay() # Instanciate a new replay replay.append(state, # Add experience to the replay action, reward, next_state, done, info={ 'density': action_density, 'log_prob': action_density.log_prob(action), }) replay.state # Tensor of states replay.actions # Tensor of actions replay.densitys # list of action_density replay.log_probs # Tensor of log_probabilities new_replay = replay[-10:] # Last 10 transitions in new_replay #Sample some previous experience batch = replay.sample(32, contiguous=True) save ExperienceReplay.save(self, path) Description Serializes and saves the ExperienceReplay into the given path. Arguments path (str) - File path. Example replay.save('my_replay_file.pt') load ExperienceReplay.load(self, path) Description Loads data from a serialized ExperienceReplay. Arguments path (str) - File path of serialized ExperienceReplay. Example replay.load('my_replay_file.pt') append ExperienceReplay.append(self, state, action, reward, next_state, done, info=None) Description Appends new data to the list ExperienceReplay. Arguments state (tensor/ndarray/list) - Originating state. action (tensor/ndarray/list) - Executed action. reward (tensor/ndarray/list) - Observed reward. next_state (tensor/ndarray/list) - Resulting state. done (tensor/bool) - Is next_state a terminal (absorbing) state ? info (dict, optional , default=None) - Additional information on the transition. Example replay.append(state, action, reward, next_state, done, info={ 'density': density, 'log_prob': density.log_prob(action), }) add ExperienceReplay.add(self, *args, **kwargs) Description (Deprecated) Alias for .append() update ExperienceReplay.update(self, fn) Description Updates all samples in the replay according to fn . fn should take two arguments and returns a dict of updated values. The first one corresponds to the index of the transition to be updated. The second is the transition itself. Note: You should return the updated values, not modify the values in-place on the transition. Arguments fn (function) - Update function. Example replay.update(lambda i, sars: { 'reward': rewards[i].detach(), 'info': { 'advantage': advantages[i].detach() }, }) sample ExperienceReplay.sample(self, size=1, contiguous=False, episodes=False) Samples from the Experience replay. Arguments size (int, optional , default=1) - The number of samples. contiguous (bool, optional , default=False) - Whether to sample contiguous transitions. episodes (bool, optional , default=False) - Sample full episodes, instead of transitions. Return ExperienceReplay - New ExperienceReplay containing the sampled transitions. empty ExperienceReplay.empty(self) Description Removes all data from an ExperienceReplay. Example replay.empty()","title":"cherry.ExperienceReplay"},{"location":"docs/cherry.experience-replay/#transition","text":"Transition(self, state, action, reward, next_state, done, info=None) Description Represents a (s, a, r, s', d) tuple. Arguments state (tensor) - Originating state. action (tensor) - Executed action. reward (tensor) - Observed reward. next_state (tensor) - Resulting state. done (tensor) - Is next_state a terminal (absorbing) state ? info (dict, optional , default=None) - Additional information on the transition. Example for transition in replay: print(transition.state)","title":"Transition"},{"location":"docs/cherry.experience-replay/#experiencereplay","text":"ExperienceReplay(self, states=None, actions=None, rewards=None, next_states=None, dones=None, infos=None) [Source] Description Experience replay buffer to store, retrieve, and sample past transitions. ExperienceReplay behaves like a list of transitions, . It also support accessing specific properties, such as states, actions, rewards, next_states, and informations. The first four are returned as tensors, while infos is returned as a list of dicts. The properties of infos can be accessed directly by appending an s to their dictionary key -- see Examples below. In this case, if the values of the infos are tensors, they will be returned as a concatenated Tensor. Otherwise, they default to a list of values. Arguments states (Tensor, optional , default=None) - Tensor of states. actions (Tensor, optional , default=None) - Tensor of actions. rewards (Tensor, optional , default=None) - Tensor of rewards. next_states (Tensor, optional , default=None) - Tensor of next_states. dones (Tensor, optional , default=None) - Tensor of dones. infos (list, optional , default=None) - List of infos. References Lin, Long-Ji. 1992. \u201cSelf-Improving Reactive Agents Based on Reinforcement Learning, Planning and Teaching.\u201d Machine Learning 8 (3): 293\u2013321. Example replay = ch.ExperienceReplay() # Instanciate a new replay replay.append(state, # Add experience to the replay action, reward, next_state, done, info={ 'density': action_density, 'log_prob': action_density.log_prob(action), }) replay.state # Tensor of states replay.actions # Tensor of actions replay.densitys # list of action_density replay.log_probs # Tensor of log_probabilities new_replay = replay[-10:] # Last 10 transitions in new_replay #Sample some previous experience batch = replay.sample(32, contiguous=True)","title":"ExperienceReplay"},{"location":"docs/cherry.experience-replay/#save","text":"ExperienceReplay.save(self, path) Description Serializes and saves the ExperienceReplay into the given path. Arguments path (str) - File path. Example replay.save('my_replay_file.pt')","title":"save"},{"location":"docs/cherry.experience-replay/#load","text":"ExperienceReplay.load(self, path) Description Loads data from a serialized ExperienceReplay. Arguments path (str) - File path of serialized ExperienceReplay. Example replay.load('my_replay_file.pt')","title":"load"},{"location":"docs/cherry.experience-replay/#append","text":"ExperienceReplay.append(self, state, action, reward, next_state, done, info=None) Description Appends new data to the list ExperienceReplay. Arguments state (tensor/ndarray/list) - Originating state. action (tensor/ndarray/list) - Executed action. reward (tensor/ndarray/list) - Observed reward. next_state (tensor/ndarray/list) - Resulting state. done (tensor/bool) - Is next_state a terminal (absorbing) state ? info (dict, optional , default=None) - Additional information on the transition. Example replay.append(state, action, reward, next_state, done, info={ 'density': density, 'log_prob': density.log_prob(action), })","title":"append"},{"location":"docs/cherry.experience-replay/#add","text":"ExperienceReplay.add(self, *args, **kwargs) Description (Deprecated) Alias for .append()","title":"add"},{"location":"docs/cherry.experience-replay/#update","text":"ExperienceReplay.update(self, fn) Description Updates all samples in the replay according to fn . fn should take two arguments and returns a dict of updated values. The first one corresponds to the index of the transition to be updated. The second is the transition itself. Note: You should return the updated values, not modify the values in-place on the transition. Arguments fn (function) - Update function. Example replay.update(lambda i, sars: { 'reward': rewards[i].detach(), 'info': { 'advantage': advantages[i].detach() }, })","title":"update"},{"location":"docs/cherry.experience-replay/#sample","text":"ExperienceReplay.sample(self, size=1, contiguous=False, episodes=False) Samples from the Experience replay. Arguments size (int, optional , default=1) - The number of samples. contiguous (bool, optional , default=False) - Whether to sample contiguous transitions. episodes (bool, optional , default=False) - Sample full episodes, instead of transitions. Return ExperienceReplay - New ExperienceReplay containing the sampled transitions.","title":"sample"},{"location":"docs/cherry.experience-replay/#empty","text":"ExperienceReplay.empty(self) Description Removes all data from an ExperienceReplay. Example replay.empty()","title":"empty"},{"location":"docs/cherry.models/","text":"cherry.models.utils polyak_average polyak_average(source, target, alpha) [Source] Description Shifts the parameters of a model towards those of another one. Note: the parameter alpha indicates how much to shift in the direction of target . (i.e. the old parameters are kept at a rate of 1 - alpha .) References Polyak, B., and A. Juditsky. 1992. \u201cAcceleration of Stochastic Approximation by Averaging.\u201d Arguments source (nn.Module) - The module to be shifted. target (nn.Module) - The module indicating the shift direction. alpha (float) - Strength of the shift. Example target_qf = nn.Linear(23, 34) qf = nn.Linear(23, 34) ch.models.polyak_average(target_qf, qf, alpha=0.1) cherry.models.tabular StateValueFunction StateValueFunction(self, state_size, init=None) [Source] Description Stores a table of state values, V(s), one for each state. Assumes that the states are one-hot encoded. Also, the returned values are differentiable and can be used in conjunction with PyTorch's optimizers. Arguments state_size (int) - The number of states in the environment. init (function, optional , default=None) - The initialization scheme for the values in the table. (Default is 0.) References Sutton, Richard, and Andrew Barto. 2018. Reinforcement Learning, Second Edition. The MIT Press. Example vf = StateValueFunction(env.state_size) state = env.reset() state = ch.utils.onehot(state, env.state_size) state_value = vf(state) ActionValueFunction ActionValueFunction(self, state_size, action_size, init=None) [Source] Description Stores a table of action values, Q(s, a), one for each (state, action) pair. Assumes that the states and actions are one-hot encoded. Also, the returned values are differentiable and can be used in conjunction with PyTorch's optimizers. Arguments state_size (int) - The number of states in the environment. action_size (int) - The number of actions per state. init (function, optional , default=None) - The initialization scheme for the values in the table. (Default is 0.) References Sutton, Richard, and Andrew Barto. 2018. Reinforcement Learning, Second Edition. The MIT Press. Example qf = ActionValueFunction(env.state_size, env.action_size) state = env.reset() state = ch.utils.onehot(state, env.state_size) all_action_values = qf(state) action = ch.utils.onehot(0, env.action_size) action_value = qf(state, action) cherry.models.atari NatureFeatures NatureFeatures(self, input_size=4, hidden_size=512) [Source] Description The convolutional body of the DQN architecture. References Mnih et al. 2015. \u201cHuman-Level Control through Deep Reinforcement Learning.\u201d Mnih et al. 2016. \u201cAsynchronous Methods for Deep Reinforcement Learning.\u201d Credit Adapted from Ilya Kostrikov's implementation. Arguments input_size (int) - Number of channels. (Stacked frames in original implementation.) output_size (int, optional , default=512) - Size of the output representation. NatureActor NatureActor(self, input_size, output_size) [Source] Description The actor head of the A3C architecture. References Mnih et al. 2015. \u201cHuman-Level Control through Deep Reinforcement Learning.\u201d Mnih et al. 2016. \u201cAsynchronous Methods for Deep Reinforcement Learning.\u201d Credit Adapted from Ilya Kostrikov's implementation. Arguments input_size (int) - Size of input of the fully connected layers output_size (int) - Size of the action space. NatureCritic NatureCritic(self, input_size, output_size=1) [Source] Description The critic head of the A3C architecture. References Mnih et al. 2015. \u201cHuman-Level Control through Deep Reinforcement Learning.\u201d Mnih et al. 2016. \u201cAsynchronous Methods for Deep Reinforcement Learning.\u201d Credit Adapted from Ilya Kostrikov's implementation. Arguments input_size (int) - Size of input of the fully connected layers output_size (int, optional , default=1) - Size of the value. cherry.models.control ControlMLP ControlMLP(self, input_size, output_size, layer_sizes=None) [Source] Description A multi-layer perceptron with proper initialization for continuous control. Credit Adapted from Ilya Kostrikov's implementation. Arguments inputs_size (int) - Size of input. output_size (int) - Size of output. layer_sizes (list, optional , default=None) - A list of ints, each indicating the size of a hidden layer. (Defaults to two hidden layers of 64 units.) Example target_qf = ch.models.control.ControlMLP(23, 34, layer_sizes=[32, 32]) Actor Actor(self, input_size, output_size, layer_sizes=None) [Source] Description A multi-layer perceptron with initialization designed for choosing actions in continuous control environments. Credit Adapted from Ilya Kostrikov's implementation. Arguments inputs_size (int) - Size of input. output_size (int) - Size of action size. layer_sizes (list, optional , default=None) - A list of ints, each indicating the size of a hidden layer. (Defaults to two hidden layers of 64 units.) Example policy_mean = ch.models.control.Actor(28, 8, layer_sizes=[64, 32, 16])","title":"cherry.models"},{"location":"docs/cherry.models/#cherrymodelsutils","text":"","title":"cherry.models.utils"},{"location":"docs/cherry.models/#polyak_average","text":"polyak_average(source, target, alpha) [Source] Description Shifts the parameters of a model towards those of another one. Note: the parameter alpha indicates how much to shift in the direction of target . (i.e. the old parameters are kept at a rate of 1 - alpha .) References Polyak, B., and A. Juditsky. 1992. \u201cAcceleration of Stochastic Approximation by Averaging.\u201d Arguments source (nn.Module) - The module to be shifted. target (nn.Module) - The module indicating the shift direction. alpha (float) - Strength of the shift. Example target_qf = nn.Linear(23, 34) qf = nn.Linear(23, 34) ch.models.polyak_average(target_qf, qf, alpha=0.1)","title":"polyak_average"},{"location":"docs/cherry.models/#cherrymodelstabular","text":"","title":"cherry.models.tabular"},{"location":"docs/cherry.models/#statevaluefunction","text":"StateValueFunction(self, state_size, init=None) [Source] Description Stores a table of state values, V(s), one for each state. Assumes that the states are one-hot encoded. Also, the returned values are differentiable and can be used in conjunction with PyTorch's optimizers. Arguments state_size (int) - The number of states in the environment. init (function, optional , default=None) - The initialization scheme for the values in the table. (Default is 0.) References Sutton, Richard, and Andrew Barto. 2018. Reinforcement Learning, Second Edition. The MIT Press. Example vf = StateValueFunction(env.state_size) state = env.reset() state = ch.utils.onehot(state, env.state_size) state_value = vf(state)","title":"StateValueFunction"},{"location":"docs/cherry.models/#actionvaluefunction","text":"ActionValueFunction(self, state_size, action_size, init=None) [Source] Description Stores a table of action values, Q(s, a), one for each (state, action) pair. Assumes that the states and actions are one-hot encoded. Also, the returned values are differentiable and can be used in conjunction with PyTorch's optimizers. Arguments state_size (int) - The number of states in the environment. action_size (int) - The number of actions per state. init (function, optional , default=None) - The initialization scheme for the values in the table. (Default is 0.) References Sutton, Richard, and Andrew Barto. 2018. Reinforcement Learning, Second Edition. The MIT Press. Example qf = ActionValueFunction(env.state_size, env.action_size) state = env.reset() state = ch.utils.onehot(state, env.state_size) all_action_values = qf(state) action = ch.utils.onehot(0, env.action_size) action_value = qf(state, action)","title":"ActionValueFunction"},{"location":"docs/cherry.models/#cherrymodelsatari","text":"","title":"cherry.models.atari"},{"location":"docs/cherry.models/#naturefeatures","text":"NatureFeatures(self, input_size=4, hidden_size=512) [Source] Description The convolutional body of the DQN architecture. References Mnih et al. 2015. \u201cHuman-Level Control through Deep Reinforcement Learning.\u201d Mnih et al. 2016. \u201cAsynchronous Methods for Deep Reinforcement Learning.\u201d Credit Adapted from Ilya Kostrikov's implementation. Arguments input_size (int) - Number of channels. (Stacked frames in original implementation.) output_size (int, optional , default=512) - Size of the output representation.","title":"NatureFeatures"},{"location":"docs/cherry.models/#natureactor","text":"NatureActor(self, input_size, output_size) [Source] Description The actor head of the A3C architecture. References Mnih et al. 2015. \u201cHuman-Level Control through Deep Reinforcement Learning.\u201d Mnih et al. 2016. \u201cAsynchronous Methods for Deep Reinforcement Learning.\u201d Credit Adapted from Ilya Kostrikov's implementation. Arguments input_size (int) - Size of input of the fully connected layers output_size (int) - Size of the action space.","title":"NatureActor"},{"location":"docs/cherry.models/#naturecritic","text":"NatureCritic(self, input_size, output_size=1) [Source] Description The critic head of the A3C architecture. References Mnih et al. 2015. \u201cHuman-Level Control through Deep Reinforcement Learning.\u201d Mnih et al. 2016. \u201cAsynchronous Methods for Deep Reinforcement Learning.\u201d Credit Adapted from Ilya Kostrikov's implementation. Arguments input_size (int) - Size of input of the fully connected layers output_size (int, optional , default=1) - Size of the value.","title":"NatureCritic"},{"location":"docs/cherry.models/#cherrymodelscontrol","text":"","title":"cherry.models.control"},{"location":"docs/cherry.models/#controlmlp","text":"ControlMLP(self, input_size, output_size, layer_sizes=None) [Source] Description A multi-layer perceptron with proper initialization for continuous control. Credit Adapted from Ilya Kostrikov's implementation. Arguments inputs_size (int) - Size of input. output_size (int) - Size of output. layer_sizes (list, optional , default=None) - A list of ints, each indicating the size of a hidden layer. (Defaults to two hidden layers of 64 units.) Example target_qf = ch.models.control.ControlMLP(23, 34, layer_sizes=[32, 32])","title":"ControlMLP"},{"location":"docs/cherry.models/#actor","text":"Actor(self, input_size, output_size, layer_sizes=None) [Source] Description A multi-layer perceptron with initialization designed for choosing actions in continuous control environments. Credit Adapted from Ilya Kostrikov's implementation. Arguments inputs_size (int) - Size of input. output_size (int) - Size of action size. layer_sizes (list, optional , default=None) - A list of ints, each indicating the size of a hidden layer. (Defaults to two hidden layers of 64 units.) Example policy_mean = ch.models.control.Actor(28, 8, layer_sizes=[64, 32, 16])","title":"Actor"},{"location":"docs/cherry.nn.init/","text":"cherry.nn.init","title":"cherry.nn.init"},{"location":"docs/cherry.nn.init/#cherrynninit","text":"","title":"cherry.nn.init"},{"location":"docs/cherry.nn/","text":"cherry.nn","title":"cherry.nn"},{"location":"docs/cherry.nn/#cherrynn","text":"","title":"cherry.nn"},{"location":"docs/cherry.optim/","text":"cherry.optim Distributed Distributed(self, params=<required parameter>, opt=<required parameter>) Synchronizes optimizers across distributed replicas.","title":"cherry.optim"},{"location":"docs/cherry.optim/#cherryoptim","text":"","title":"cherry.optim"},{"location":"docs/cherry.optim/#distributed","text":"Distributed(self, params=<required parameter>, opt=<required parameter>) Synchronizes optimizers across distributed replicas.","title":"Distributed"},{"location":"docs/cherry.plot/","text":"ci95 ci95(values) Description Computes the 95% confidence interval around the given values. Arguments values (list) - List of values for which to compute the 95% confidence interval. Returns (float, float) The lower and upper bounds of the confidence interval. Example from statistics import mean smoothed = [] for replay in replays: rewards = replay.rewards.view(-1).tolist() x, y_smoothed = ch.plot.smooth(rewards) smoothed.append(y_smoothed) means = [mean(r) for r in zip(*smoothed)] confidences = [ch.plot.ci95(r) for r in zip(*smoothed)] lower_bound = [conf[0] for conf in confidences] upper_bound = [conf[1] for conf in confidences] exponential_smoothing exponential_smoothing(x, y=None, temperature=1.0) [Source] Decription Two side exponential moving average for smoothing a curve It performs regular exponential moving average twice from two different sides and then combines the results together. It performs better than one side exponential smoothing. Arguments x (ndarray/tensor/list) - x values, in accending order. y (ndarray/tensor/list) - y values. temperature (float, optional , default=1.0) - The higher, the smoother. Return ndarray - x values after resampling. ndarray - y values after smoothing. Credit Adapted from OpenAI's baselines implementation. Example from cherry.plot import exponential_smoothing x_smoothed, y_smoothed, _ = exponential_smoothing(x_original, y_original, temperature=3.)","title":"cherry.plot"},{"location":"docs/cherry.plot/#ci95","text":"ci95(values) Description Computes the 95% confidence interval around the given values. Arguments values (list) - List of values for which to compute the 95% confidence interval. Returns (float, float) The lower and upper bounds of the confidence interval. Example from statistics import mean smoothed = [] for replay in replays: rewards = replay.rewards.view(-1).tolist() x, y_smoothed = ch.plot.smooth(rewards) smoothed.append(y_smoothed) means = [mean(r) for r in zip(*smoothed)] confidences = [ch.plot.ci95(r) for r in zip(*smoothed)] lower_bound = [conf[0] for conf in confidences] upper_bound = [conf[1] for conf in confidences]","title":"ci95"},{"location":"docs/cherry.plot/#exponential_smoothing","text":"exponential_smoothing(x, y=None, temperature=1.0) [Source] Decription Two side exponential moving average for smoothing a curve It performs regular exponential moving average twice from two different sides and then combines the results together. It performs better than one side exponential smoothing. Arguments x (ndarray/tensor/list) - x values, in accending order. y (ndarray/tensor/list) - y values. temperature (float, optional , default=1.0) - The higher, the smoother. Return ndarray - x values after resampling. ndarray - y values after smoothing. Credit Adapted from OpenAI's baselines implementation. Example from cherry.plot import exponential_smoothing x_smoothed, y_smoothed, _ = exponential_smoothing(x_original, y_original, temperature=3.)","title":"exponential_smoothing"},{"location":"docs/cherry.rewards/","text":"cherry.rewards [Source] Description Utilities to manipulate rewards, such as discounting or advantage computation. discount discount(gamma, rewards, dones, bootstrap=0.0) Description Discounts rewards at an rate of gamma. References Sutton, Richard, and Andrew Barto. 2018. Reinforcement Learning, Second Edition. The MIT Press. Arguments gamma (float) - Discount factor. rewards (tensor) - Tensor of rewards. dones (tensor) - Tensor indicating episode termination. Entry is 1 if the transition led to a terminal (absorbing) state, 0 else. bootstrap (float, optional , default=0.0) - Bootstrap the last reward with this value. Returns tensor - Tensor of discounted rewards. Example rewards = th.ones(23, 1) * 8 dones = th.zeros_like(rewards) dones[-1] += 1.0 discounted = ch.rewards.discount(0.99, rewards, dones, bootstrap=1.0) generalized_advantage generalized_advantage(gamma, tau, rewards, dones, values, next_value) Description Computes the generalized advantage estimator. (GAE) References Schulman et al. 2015. \u201cHigh-Dimensional Continuous Control Using Generalized Advantage Estimation\u201d https://github.com/joschu/modular_rl/blob/master/modular_rl/core.py#L49 Arguments gamma (float) - Discount factor. tau (float) - Bias-variance trade-off. rewards (tensor) - Tensor of rewards. dones (tensor) - Tensor indicating episode termination. Entry is 1 if the transition led to a terminal (absorbing) state, 0 else. values (tensor) - Values for the states producing the rewards. next_value (tensor) - Value of the state obtained after the transition from the state used to compute the last value in values . Returns tensor - Tensor of advantages. Example mass, next_value = policy(replay.next_states[-1]) advantages = generalized_advantage(0.99, 0.95, replay.) temporal_difference temporal_difference(gamma, rewards, dones, values, next_value) Description Returns the temporal difference residual. Reference Sutton, Richard S. 1988. \u201cLearning to Predict by the Methods of Temporal Differences.\u201d Machine Learning 3 (1): 9\u201344. Sutton, Richard, and Andrew Barto. 2018. Reinforcement Learning, Second Edition. The MIT Press. Arguments gamma (float) - Discount factor. rewards (tensor) - Tensor of rewards. dones (tensor) - Tensor indicating episode termination. Entry is 1 if the transition led to a terminal (absorbing) state, 0 else. values (tensor) - Values for the states producing the rewards. next_value (tensor) - Value of the state obtained after the transition from the state used to compute the last value in values . Example value = vf(state) td_errors = temporal_difference(0.99, replay.rewards, replay.dones, replay.values, next_value)","title":"cherry.rewards"},{"location":"docs/cherry.rewards/#cherryrewards","text":"[Source] Description Utilities to manipulate rewards, such as discounting or advantage computation.","title":"cherry.rewards"},{"location":"docs/cherry.rewards/#discount","text":"discount(gamma, rewards, dones, bootstrap=0.0) Description Discounts rewards at an rate of gamma. References Sutton, Richard, and Andrew Barto. 2018. Reinforcement Learning, Second Edition. The MIT Press. Arguments gamma (float) - Discount factor. rewards (tensor) - Tensor of rewards. dones (tensor) - Tensor indicating episode termination. Entry is 1 if the transition led to a terminal (absorbing) state, 0 else. bootstrap (float, optional , default=0.0) - Bootstrap the last reward with this value. Returns tensor - Tensor of discounted rewards. Example rewards = th.ones(23, 1) * 8 dones = th.zeros_like(rewards) dones[-1] += 1.0 discounted = ch.rewards.discount(0.99, rewards, dones, bootstrap=1.0)","title":"discount"},{"location":"docs/cherry.rewards/#generalized_advantage","text":"generalized_advantage(gamma, tau, rewards, dones, values, next_value) Description Computes the generalized advantage estimator. (GAE) References Schulman et al. 2015. \u201cHigh-Dimensional Continuous Control Using Generalized Advantage Estimation\u201d https://github.com/joschu/modular_rl/blob/master/modular_rl/core.py#L49 Arguments gamma (float) - Discount factor. tau (float) - Bias-variance trade-off. rewards (tensor) - Tensor of rewards. dones (tensor) - Tensor indicating episode termination. Entry is 1 if the transition led to a terminal (absorbing) state, 0 else. values (tensor) - Values for the states producing the rewards. next_value (tensor) - Value of the state obtained after the transition from the state used to compute the last value in values . Returns tensor - Tensor of advantages. Example mass, next_value = policy(replay.next_states[-1]) advantages = generalized_advantage(0.99, 0.95, replay.)","title":"generalized_advantage"},{"location":"docs/cherry.rewards/#temporal_difference","text":"temporal_difference(gamma, rewards, dones, values, next_value) Description Returns the temporal difference residual. Reference Sutton, Richard S. 1988. \u201cLearning to Predict by the Methods of Temporal Differences.\u201d Machine Learning 3 (1): 9\u201344. Sutton, Richard, and Andrew Barto. 2018. Reinforcement Learning, Second Edition. The MIT Press. Arguments gamma (float) - Discount factor. rewards (tensor) - Tensor of rewards. dones (tensor) - Tensor indicating episode termination. Entry is 1 if the transition led to a terminal (absorbing) state, 0 else. values (tensor) - Values for the states producing the rewards. next_value (tensor) - Value of the state obtained after the transition from the state used to compute the last value in values . Example value = vf(state) td_errors = temporal_difference(0.99, replay.rewards, replay.dones, replay.values, next_value)","title":"temporal_difference"},{"location":"docs/cherry.utils/","text":"cherry.utils min_size min_size(tensor) Returns the minimium viewable size of a tensor. e.g. (1, 1, 3, 4) -> (3, 4) normalize normalize(tensor, epsilon=1e-08) Normalizes a tensor to zero mean and unit std. onehot onehot(x, dim) Creates a new onehot encoded tensor.","title":"cherry.utils"},{"location":"docs/cherry.utils/#cherryutils","text":"","title":"cherry.utils"},{"location":"docs/cherry.utils/#min_size","text":"min_size(tensor) Returns the minimium viewable size of a tensor. e.g. (1, 1, 3, 4) -> (3, 4)","title":"min_size"},{"location":"docs/cherry.utils/#normalize","text":"normalize(tensor, epsilon=1e-08) Normalizes a tensor to zero mean and unit std.","title":"normalize"},{"location":"docs/cherry.utils/#onehot","text":"onehot(x, dim) Creates a new onehot encoded tensor.","title":"onehot"},{"location":"tutorials/debugging_rl/","text":"Debugging Reinforcement Learning Objectives: Show how cherry helps you catch bugs with defensive programming, Show how to use the VisdomLogger and the Record wrapper Provide some tips and tricks on debugging.","title":"Debugging Reinforcement Learning"},{"location":"tutorials/debugging_rl/#debugging-reinforcement-learning","text":"Objectives: Show how cherry helps you catch bugs with defensive programming, Show how to use the VisdomLogger and the Record wrapper Provide some tips and tricks on debugging.","title":"Debugging Reinforcement Learning"},{"location":"tutorials/distributed_ppo/","text":"Distributed Training with PPO Objectives: Show how to use the control models, Show how to setup the distributed optimizer, Showcase the PPO methods. Pretty short too.","title":"Distributed Training with PPO"},{"location":"tutorials/distributed_ppo/#distributed-training-with-ppo","text":"Objectives: Show how to use the control models, Show how to setup the distributed optimizer, Showcase the PPO methods. Pretty short too.","title":"Distributed Training with PPO"},{"location":"tutorials/getting_started/","text":"Getting Started with Cherry Install Explain philosophy Overview of tools Use for a very simple examples: reinforce and q-learning on debug grid world.","title":"Getting Started with Cherry"},{"location":"tutorials/getting_started/#getting-started-with-cherry","text":"Install Explain philosophy Overview of tools Use for a very simple examples: reinforce and q-learning on debug grid world.","title":"Getting Started with Cherry"},{"location":"tutorials/recurrent_a2c/","text":"Recurrent Policy Gradients with A2C Objectives: Show how to use Atari models + wrappers. Show how to implement recurrent policy gradients with cherry. That's it, very short.","title":"Recurrent Policy Gradients with A2C"},{"location":"tutorials/recurrent_a2c/#recurrent-policy-gradients-with-a2c","text":"Objectives: Show how to use Atari models + wrappers. Show how to implement recurrent policy gradients with cherry. That's it, very short.","title":"Recurrent Policy Gradients with A2C"}]}