{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Cherry is a reinforcement learning framework for researchers built on top of PyTorch. Unlike other reinforcement learning implementations, cherry doesn't implement a single monolithic interface to existing algorithms. Instead, it provides you with low-level, common tools to write your own algorithms. Drawing from the UNIX philosophy, each tool strives to be as independent from the rest of the framework as possible. So if you don't like a specific tool, you don\u2019t need to use it. Features Pythonic and low-level interface \u00e0 la Pytorch. Support for tabular (!) and function approximation algorithms. Various OpenAI Gym environment wrappers. Helper functions for popular algorithms. (e.g. A2C, DDPG, TRPO, PPO, SAC) Logging, visualization, and debugging tools. Painless and efficient distributed training on CPUs and GPUs. Unit, integration, and regression tested, continuously integrated. To learn more about the tools and philosophy behind cherry, check out our Getting Started tutorial . Example The following snippet showcases some of the tools offered by cherry. import cherry as ch # Wrap environments env = gym.make('CartPole-v0') env = ch.envs.Logger(env, interval=1000) env = ch.envs.Torch(env) policy = PolicyNet() optimizer = optim.Adam(policy.parameters(), lr=1e-2) replay = ch.ExperienceReplay() # Manage transitions for step in range(1000): state = env.reset() while True: mass = Categorical(policy(state)) action = mass.sample() log_prob = mass.log_prob(action) next_state, reward, done, _ = env.step(action) # Build the ExperienceReplay replay.append(state, action, reward, next_state, done, log_prob=log_prob) if done: break else: state = next_state # Discounting and normalizing rewards rewards = ch.td.discount(0.99, replay.reward(), replay.done()) rewards = ch.normalize(rewards) loss = -th.sum(replay.log_prob() * rewards) optimizer.zero_grad() loss.backward() optimizer.step() replay.empty() Many more high-quality examples are available in the examples/ folder. Installation Note Cherry is considered in early alpha release. Stuff might break. pip install cherry-rl Documentation Documentation and tutorials are available on cherry\u2019s website: http://cherry-rl.net . Contributing First, thanks for your consideration in contributing to cherry. Here are a couple of guidelines we strive to follow. It's always a good idea to open an issue first, where we can discuss how to best proceed. Branch/fork from dev , and create a pull request as soon as possible to allow for early discussions. If you want to contribute a new example using cherry, it would preferably stand in a single file. If you would like to contribute a new feature to the core library, we suggest to first implement an example showcasing your new functionality. Doing so is quite useful: it allows for automatic testing, it ensures that the functionality is correctly implemented, it shows users how to use your functionality, and it gives a concrete example when discussing the best way to merge your implementation. We don't have forums, but are happy to discuss with you on slack. Make sure to send an email to smr.arnold@gmail.com to get an invite. Acknowledgements Cherry draws inspiration from many reinforcement learning implementations, including OpenAI Baselines , John Schulman's implementations Ilya Kostrikov's implementations , Shangtong Zhang's implementations , Dave Abel's implementations , Vitchyr Pong's implementations , Kai Arulkumaran's implementations , RLLab / Garage . Why 'cherry' ? Because it's the sweetest part of the cake .","title":"Home"},{"location":"#example","text":"The following snippet showcases some of the tools offered by cherry. import cherry as ch # Wrap environments env = gym.make('CartPole-v0') env = ch.envs.Logger(env, interval=1000) env = ch.envs.Torch(env) policy = PolicyNet() optimizer = optim.Adam(policy.parameters(), lr=1e-2) replay = ch.ExperienceReplay() # Manage transitions for step in range(1000): state = env.reset() while True: mass = Categorical(policy(state)) action = mass.sample() log_prob = mass.log_prob(action) next_state, reward, done, _ = env.step(action) # Build the ExperienceReplay replay.append(state, action, reward, next_state, done, log_prob=log_prob) if done: break else: state = next_state # Discounting and normalizing rewards rewards = ch.td.discount(0.99, replay.reward(), replay.done()) rewards = ch.normalize(rewards) loss = -th.sum(replay.log_prob() * rewards) optimizer.zero_grad() loss.backward() optimizer.step() replay.empty() Many more high-quality examples are available in the examples/ folder.","title":"Example"},{"location":"#installation","text":"Note Cherry is considered in early alpha release. Stuff might break. pip install cherry-rl","title":"Installation"},{"location":"#documentation","text":"Documentation and tutorials are available on cherry\u2019s website: http://cherry-rl.net .","title":"Documentation"},{"location":"#contributing","text":"First, thanks for your consideration in contributing to cherry. Here are a couple of guidelines we strive to follow. It's always a good idea to open an issue first, where we can discuss how to best proceed. Branch/fork from dev , and create a pull request as soon as possible to allow for early discussions. If you want to contribute a new example using cherry, it would preferably stand in a single file. If you would like to contribute a new feature to the core library, we suggest to first implement an example showcasing your new functionality. Doing so is quite useful: it allows for automatic testing, it ensures that the functionality is correctly implemented, it shows users how to use your functionality, and it gives a concrete example when discussing the best way to merge your implementation. We don't have forums, but are happy to discuss with you on slack. Make sure to send an email to smr.arnold@gmail.com to get an invite.","title":"Contributing"},{"location":"#acknowledgements","text":"Cherry draws inspiration from many reinforcement learning implementations, including OpenAI Baselines , John Schulman's implementations Ilya Kostrikov's implementations , Shangtong Zhang's implementations , Dave Abel's implementations , Vitchyr Pong's implementations , Kai Arulkumaran's implementations , RLLab / Garage .","title":"Acknowledgements"},{"location":"#why-cherry","text":"Because it's the sweetest part of the cake .","title":"Why 'cherry' ?"},{"location":"docs/cherry.algorithms/","text":"cherry.algorithms.a2c Helper functions for A2C. policy_loss policy_loss(log_probs, advantages) [Source] Description Advantage Actor-Critic policy loss. References Arguments Returns Example advantages = replay.advantages log_probs = replay.log_probs loss = a2c.policy_loss(log_probs, advantages) state_value_loss state_value_loss(values, rewards) Advantage Actor-Critic value loss. cherry.algorithms.ppo policy_loss policy_loss(new_log_probs, old_log_probs, advantages, clip=0.1) Clipped policy loss. Arguments: new_log_probs: (tensor) old_log_probs: (tensor) advantages: (tensor) clip: (tensor) state_value_loss state_value_loss(new_values, old_values, rewards, clip=0.1) Clipped value loss. Arguments: new_values: (tensor) old_values: (tensor) rewards: (tensor) clip: (tensor) cherry.algorithms.sac policy_loss policy_loss(log_probs, q_actions, alpha=1.0) Arguments: log_probs: the log density of the actions from the current policy on some states. q_actions: the Q-values for those same actions. alpha: the weight of the policy entropy. action_value_loss action_value_loss(q_old_pred, v_next, rewards, dones, gamma) Arguments: q_old_pred: Q values on an existing transition. v_next: V values for the resulting state. rewards: observed rewards during the transition. dones: which states were terminal. gamma: discount factor. state_value_loss state_value_loss(v_pred, log_probs, q_values, alpha=1.0) Arguments: v_pred: the V values of states from a batch. log_probs: the log density of actions from the current policy on those states. q_values: the Q values of the those actions on those states. alpha: the weight of the policy entropy.","title":"cherry.algorithms"},{"location":"docs/cherry.algorithms/#cherryalgorithmsa2c","text":"Helper functions for A2C.","title":"cherry.algorithms.a2c"},{"location":"docs/cherry.algorithms/#policy_loss","text":"policy_loss(log_probs, advantages) [Source] Description Advantage Actor-Critic policy loss. References Arguments Returns Example advantages = replay.advantages log_probs = replay.log_probs loss = a2c.policy_loss(log_probs, advantages)","title":"policy_loss"},{"location":"docs/cherry.algorithms/#state_value_loss","text":"state_value_loss(values, rewards) Advantage Actor-Critic value loss.","title":"state_value_loss"},{"location":"docs/cherry.algorithms/#cherryalgorithmsppo","text":"","title":"cherry.algorithms.ppo"},{"location":"docs/cherry.algorithms/#policy_loss_1","text":"policy_loss(new_log_probs, old_log_probs, advantages, clip=0.1) Clipped policy loss. Arguments: new_log_probs: (tensor) old_log_probs: (tensor) advantages: (tensor) clip: (tensor)","title":"policy_loss"},{"location":"docs/cherry.algorithms/#state_value_loss_1","text":"state_value_loss(new_values, old_values, rewards, clip=0.1) Clipped value loss. Arguments: new_values: (tensor) old_values: (tensor) rewards: (tensor) clip: (tensor)","title":"state_value_loss"},{"location":"docs/cherry.algorithms/#cherryalgorithmssac","text":"","title":"cherry.algorithms.sac"},{"location":"docs/cherry.algorithms/#policy_loss_2","text":"policy_loss(log_probs, q_actions, alpha=1.0) Arguments: log_probs: the log density of the actions from the current policy on some states. q_actions: the Q-values for those same actions. alpha: the weight of the policy entropy.","title":"policy_loss"},{"location":"docs/cherry.algorithms/#action_value_loss","text":"action_value_loss(q_old_pred, v_next, rewards, dones, gamma) Arguments: q_old_pred: Q values on an existing transition. v_next: V values for the resulting state. rewards: observed rewards during the transition. dones: which states were terminal. gamma: discount factor.","title":"action_value_loss"},{"location":"docs/cherry.algorithms/#state_value_loss_2","text":"state_value_loss(v_pred, log_probs, q_values, alpha=1.0) Arguments: v_pred: the V values of states from a batch. log_probs: the log density of actions from the current policy on those states. q_values: the Q values of the those actions on those states. alpha: the weight of the policy entropy.","title":"state_value_loss"},{"location":"docs/cherry.distributions/","text":"cherry.distributions Description A set of common distributions. Reparameterization Reparameterization(self, density) [Source] Description Reparameterized distribution. References Arguments Returns Example ActionDistribution ActionDistribution(self, env, logstd=None, use_probs=False, reparam=False) [Source] Description A helper module to automatically choose the proper policy distribution, based on the environment action_space. References Arguments Returns Example TanhNormal TanhNormal(self, normal_mean, normal_std) [Source] Description Adapted from Vitchyr Pong's RLkit: https://github.com/vitchyr/rlkit/blob/master/rlkit/torch/distributions.py References Arguments Returns Example","title":"cherry.distributions"},{"location":"docs/cherry.distributions/#cherrydistributions","text":"Description A set of common distributions.","title":"cherry.distributions"},{"location":"docs/cherry.distributions/#reparameterization","text":"Reparameterization(self, density) [Source] Description Reparameterized distribution. References Arguments Returns Example","title":"Reparameterization"},{"location":"docs/cherry.distributions/#actiondistribution","text":"ActionDistribution(self, env, logstd=None, use_probs=False, reparam=False) [Source] Description A helper module to automatically choose the proper policy distribution, based on the environment action_space. References Arguments Returns Example","title":"ActionDistribution"},{"location":"docs/cherry.distributions/#tanhnormal","text":"TanhNormal(self, normal_mean, normal_std) [Source] Description Adapted from Vitchyr Pong's RLkit: https://github.com/vitchyr/rlkit/blob/master/rlkit/torch/distributions.py References Arguments Returns Example","title":"TanhNormal"},{"location":"docs/cherry.envs/","text":"cherry.envs.utils get_space_dimension get_space_dimension(space) Returns the number of elements of a space sample, when unrolled. Wrapper Wrapper(self, env) This class allows to chain Environment Wrappers while still being able to access the properties of wrapped wrappers. Example: env = gym.make('MyEnv-v0') env = envs.Logger(env) env = envs.Runner(env) env.log('asdf', 23) # Uses log() method from envs.Logger. Runner Runner(self, env) Runner wrapper. run Runner.run(self, get_action, steps=None, episodes=None, render=False) Runner wrapper's run method. Logger Logger(self, env, interval=1000, episode_interval=10, title=None) Tracks and prints some common statistics about the environment. Recorder Recorder(self, env, directory='./videos/', format='gif', suffix=None) [Source] Description Wrapper to record episodes from a rollout. Supports GIF and MP4 encoding. Arguments env (Environment) - Environment to record. directory (str, optional , default='./videos/') - Relative path to where videos will be saved. format (str, optional , default='gif') - Format of the videos. Choose in ['gif', 'mp4'], defaults to gif. If it's text environment, the format will be json. suffix (str, optional , default=None): A unique id used as part of the suffix for the file. By default, uses os.getpid(). Credit Adapted from OpenAI Gym's Monitor wrapper. Example env = gym.make('CartPole-v0') env = envs.Recorder(record_env, './videos/', format='gif') env = envs.Runner(env) env.run(get_action, episodes=3, render=True) close Recorder.close(self) Flush all monitor data to disk and close any open rending windows VisdomLogger VisdomLogger(self, env, interval=1000, episode_interval=100, name=None) Enables logging and debug values to Visdom. Arguments env: The environment to wrap. interval: (int) Update frequency for episodes. Torch Torch(self, env) This wrapper converts * actions from Tensors to numpy, * states from lists/numpy to Tensors. Example: action = Categorical(Tensor([1, 2, 3])).sample() env.step(action) Normalizer Normalizer(self, env, states=True, rewards=True, clip_states=10.0, clip_rewards=10.0, gamma=0.99, eps=1e-08) [Source] Description Normalizes the states and rewards with a running average. Arguments env (Environment) - Environment to normalize. states (bool, optional , default=True) - Whether to normalize the states. rewards (bool, optional , default=True) - Whether to normalize the rewards. clip_states (bool, optional , default=10.0) - Clip each state dimension between [-clip_states, clip_states]. clip_rewards (float, optional , default=10.0) - Clip rewards between [-clip_rewards, clip_rewards]. gamma (float, optional , default=0.99) - Discount factor for rewards running averages. eps (float, optional , default=1e-8) - Numerical stability. Credit Adapted from OpenAI's baselines implementation. Example env = gym.make('CartPole-v0') env = cherry.envs.Normalizer(env, states=True, rewards=False) RewardClipper RewardClipper(self, env) reward RewardClipper.reward(self, reward) Bin reward to {+1, 0, -1} by its sign. Monitor Monitor(self, env, directory, *args, **kwargs) Sugar coating on top of Gym's Monitor. OpenAIAtari OpenAIAtari(self, env) AddTimestep AddTimestep(self, env=None) Adds a timestep information to the state input. Modified from Ilya Kostrikov's implementation: https://github.com/ikostrikov/pytorch-a2c-ppo-acktr/ StateLambda StateLambda(self, env, fn) ActionSpaceScaler ActionSpaceScaler(self, env, clip=1.0) Scales the action space to be in the range (-clip, clip). Adapted from Vitchyr Pong's RLkit: https://github.com/vitchyr/rlkit/blob/master/rlkit/envs/wrappers.py#L41","title":"cherry.envs"},{"location":"docs/cherry.envs/#cherryenvsutils","text":"","title":"cherry.envs.utils"},{"location":"docs/cherry.envs/#get_space_dimension","text":"get_space_dimension(space) Returns the number of elements of a space sample, when unrolled.","title":"get_space_dimension"},{"location":"docs/cherry.envs/#wrapper","text":"Wrapper(self, env) This class allows to chain Environment Wrappers while still being able to access the properties of wrapped wrappers. Example: env = gym.make('MyEnv-v0') env = envs.Logger(env) env = envs.Runner(env) env.log('asdf', 23) # Uses log() method from envs.Logger.","title":"Wrapper"},{"location":"docs/cherry.envs/#runner","text":"Runner(self, env) Runner wrapper.","title":"Runner"},{"location":"docs/cherry.envs/#run","text":"Runner.run(self, get_action, steps=None, episodes=None, render=False) Runner wrapper's run method.","title":"run"},{"location":"docs/cherry.envs/#logger","text":"Logger(self, env, interval=1000, episode_interval=10, title=None) Tracks and prints some common statistics about the environment.","title":"Logger"},{"location":"docs/cherry.envs/#recorder","text":"Recorder(self, env, directory='./videos/', format='gif', suffix=None) [Source] Description Wrapper to record episodes from a rollout. Supports GIF and MP4 encoding. Arguments env (Environment) - Environment to record. directory (str, optional , default='./videos/') - Relative path to where videos will be saved. format (str, optional , default='gif') - Format of the videos. Choose in ['gif', 'mp4'], defaults to gif. If it's text environment, the format will be json. suffix (str, optional , default=None): A unique id used as part of the suffix for the file. By default, uses os.getpid(). Credit Adapted from OpenAI Gym's Monitor wrapper. Example env = gym.make('CartPole-v0') env = envs.Recorder(record_env, './videos/', format='gif') env = envs.Runner(env) env.run(get_action, episodes=3, render=True)","title":"Recorder"},{"location":"docs/cherry.envs/#close","text":"Recorder.close(self) Flush all monitor data to disk and close any open rending windows","title":"close"},{"location":"docs/cherry.envs/#visdomlogger","text":"VisdomLogger(self, env, interval=1000, episode_interval=100, name=None) Enables logging and debug values to Visdom. Arguments env: The environment to wrap. interval: (int) Update frequency for episodes.","title":"VisdomLogger"},{"location":"docs/cherry.envs/#torch","text":"Torch(self, env) This wrapper converts * actions from Tensors to numpy, * states from lists/numpy to Tensors. Example: action = Categorical(Tensor([1, 2, 3])).sample() env.step(action)","title":"Torch"},{"location":"docs/cherry.envs/#normalizer","text":"Normalizer(self, env, states=True, rewards=True, clip_states=10.0, clip_rewards=10.0, gamma=0.99, eps=1e-08) [Source] Description Normalizes the states and rewards with a running average. Arguments env (Environment) - Environment to normalize. states (bool, optional , default=True) - Whether to normalize the states. rewards (bool, optional , default=True) - Whether to normalize the rewards. clip_states (bool, optional , default=10.0) - Clip each state dimension between [-clip_states, clip_states]. clip_rewards (float, optional , default=10.0) - Clip rewards between [-clip_rewards, clip_rewards]. gamma (float, optional , default=0.99) - Discount factor for rewards running averages. eps (float, optional , default=1e-8) - Numerical stability. Credit Adapted from OpenAI's baselines implementation. Example env = gym.make('CartPole-v0') env = cherry.envs.Normalizer(env, states=True, rewards=False)","title":"Normalizer"},{"location":"docs/cherry.envs/#rewardclipper","text":"RewardClipper(self, env)","title":"RewardClipper"},{"location":"docs/cherry.envs/#reward","text":"RewardClipper.reward(self, reward) Bin reward to {+1, 0, -1} by its sign.","title":"reward"},{"location":"docs/cherry.envs/#monitor","text":"Monitor(self, env, directory, *args, **kwargs) Sugar coating on top of Gym's Monitor.","title":"Monitor"},{"location":"docs/cherry.envs/#openaiatari","text":"OpenAIAtari(self, env)","title":"OpenAIAtari"},{"location":"docs/cherry.envs/#addtimestep","text":"AddTimestep(self, env=None) Adds a timestep information to the state input. Modified from Ilya Kostrikov's implementation: https://github.com/ikostrikov/pytorch-a2c-ppo-acktr/","title":"AddTimestep"},{"location":"docs/cherry.envs/#statelambda","text":"StateLambda(self, env, fn)","title":"StateLambda"},{"location":"docs/cherry.envs/#actionspacescaler","text":"ActionSpaceScaler(self, env, clip=1.0) Scales the action space to be in the range (-clip, clip). Adapted from Vitchyr Pong's RLkit: https://github.com/vitchyr/rlkit/blob/master/rlkit/envs/wrappers.py#L41","title":"ActionSpaceScaler"},{"location":"docs/cherry/","text":"Transition Transition(self, state, action, reward, next_state, done, device=None, **infos) Description Represents a (s, a, r, s', d) tuple. All attributes (including the ones in infos) are accessible via transition.name_of_attr . (e.g. transition.log_prob if log_prob is in infos .) Arguments state (tensor) - Originating state. action (tensor) - Executed action. reward (tensor) - Observed reward. next_state (tensor) - Resulting state. done (tensor) - Is next_state a terminal (absorbing) state ? infos (dict, optional , default=None) - Additional information on the transition. Example for transition in replay: print(transition.state) ExperienceReplay ExperienceReplay(self, storage=None, device=None) [Source] Description Experience replay buffer to store, retrieve, and sample past transitions. ExperienceReplay behaves like a list of transitions, . It also support accessing specific properties, such as states, actions, rewards, next_states, and informations. The first four are returned as tensors, while infos is returned as a list of dicts. The properties of infos can be accessed directly by appending an s to their dictionary key -- see Examples below. In this case, if the values of the infos are tensors, they will be returned as a concatenated Tensor. Otherwise, they default to a list of values. Arguments states (Tensor, optional , default=None) - Tensor of states. actions (Tensor, optional , default=None) - Tensor of actions. rewards (Tensor, optional , default=None) - Tensor of rewards. next_states (Tensor, optional , default=None) - Tensor of next_states. dones (Tensor, optional , default=None) - Tensor of dones. infos (list, optional , default=None) - List of infos. References Lin, Long-Ji. 1992. \u201cSelf-Improving Reactive Agents Based on Reinforcement Learning, Planning and Teaching.\u201d Machine Learning 8 (3): 293\u2013321. Example replay = ch.ExperienceReplay() # Instanciate a new replay replay.append(state, # Add experience to the replay action, reward, next_state, done, density: action_density, log_prob: action_density.log_prob(action), ) replay.state() # Tensor of states replay.action() # Tensor of actions replay.density() # list of action_density replay.log_prob() # Tensor of log_probabilities new_replay = replay[-10:] # Last 10 transitions in new_replay #Sample some previous experience batch = replay.sample(32, contiguous=True) save ExperienceReplay.save(self, path) Description Serializes and saves the ExperienceReplay into the given path. Arguments path (str) - File path. Example replay.save('my_replay_file.pt') load ExperienceReplay.load(self, path) Description Loads data from a serialized ExperienceReplay. Arguments path (str) - File path of serialized ExperienceReplay. Example replay.load('my_replay_file.pt') append ExperienceReplay.append(self, state=None, action=None, reward=None, next_state=None, done=None, **infos) Description Appends new data to the list ExperienceReplay. Arguments state (tensor/ndarray/list) - Originating state. action (tensor/ndarray/list) - Executed action. reward (tensor/ndarray/list) - Observed reward. next_state (tensor/ndarray/list) - Resulting state. done (tensor/bool) - Is next_state a terminal (absorbing) state ? infos (dict, optional , default=None) - Additional information on the transition. Example replay.append(state, action, reward, next_state, done, info={ 'density': density, 'log_prob': density.log_prob(action), }) sample ExperienceReplay.sample(self, size=1, contiguous=False, episodes=False) Samples from the Experience replay. Arguments size (int, optional , default=1) - The number of samples. contiguous (bool, optional , default=False) - Whether to sample contiguous transitions. episodes (bool, optional , default=False) - Sample full episodes, instead of transitions. Return ExperienceReplay - New ExperienceReplay containing the sampled transitions. empty ExperienceReplay.empty(self) Description Removes all data from an ExperienceReplay. Example replay.empty() totensor totensor(array, dtype=None) [Source] Description Returns the minimium viewable size of a tensor. e.g. (1, 1, 3, 4) -> (3, 4) References Arguments Returns Example normalize normalize(tensor, epsilon=1e-08) [Source] Description Normalizes a tensor to zero mean and unit std. References Arguments Returns Example","title":"cherry"},{"location":"docs/cherry/#transition","text":"Transition(self, state, action, reward, next_state, done, device=None, **infos) Description Represents a (s, a, r, s', d) tuple. All attributes (including the ones in infos) are accessible via transition.name_of_attr . (e.g. transition.log_prob if log_prob is in infos .) Arguments state (tensor) - Originating state. action (tensor) - Executed action. reward (tensor) - Observed reward. next_state (tensor) - Resulting state. done (tensor) - Is next_state a terminal (absorbing) state ? infos (dict, optional , default=None) - Additional information on the transition. Example for transition in replay: print(transition.state)","title":"Transition"},{"location":"docs/cherry/#experiencereplay","text":"ExperienceReplay(self, storage=None, device=None) [Source] Description Experience replay buffer to store, retrieve, and sample past transitions. ExperienceReplay behaves like a list of transitions, . It also support accessing specific properties, such as states, actions, rewards, next_states, and informations. The first four are returned as tensors, while infos is returned as a list of dicts. The properties of infos can be accessed directly by appending an s to their dictionary key -- see Examples below. In this case, if the values of the infos are tensors, they will be returned as a concatenated Tensor. Otherwise, they default to a list of values. Arguments states (Tensor, optional , default=None) - Tensor of states. actions (Tensor, optional , default=None) - Tensor of actions. rewards (Tensor, optional , default=None) - Tensor of rewards. next_states (Tensor, optional , default=None) - Tensor of next_states. dones (Tensor, optional , default=None) - Tensor of dones. infos (list, optional , default=None) - List of infos. References Lin, Long-Ji. 1992. \u201cSelf-Improving Reactive Agents Based on Reinforcement Learning, Planning and Teaching.\u201d Machine Learning 8 (3): 293\u2013321. Example replay = ch.ExperienceReplay() # Instanciate a new replay replay.append(state, # Add experience to the replay action, reward, next_state, done, density: action_density, log_prob: action_density.log_prob(action), ) replay.state() # Tensor of states replay.action() # Tensor of actions replay.density() # list of action_density replay.log_prob() # Tensor of log_probabilities new_replay = replay[-10:] # Last 10 transitions in new_replay #Sample some previous experience batch = replay.sample(32, contiguous=True)","title":"ExperienceReplay"},{"location":"docs/cherry/#save","text":"ExperienceReplay.save(self, path) Description Serializes and saves the ExperienceReplay into the given path. Arguments path (str) - File path. Example replay.save('my_replay_file.pt')","title":"save"},{"location":"docs/cherry/#load","text":"ExperienceReplay.load(self, path) Description Loads data from a serialized ExperienceReplay. Arguments path (str) - File path of serialized ExperienceReplay. Example replay.load('my_replay_file.pt')","title":"load"},{"location":"docs/cherry/#append","text":"ExperienceReplay.append(self, state=None, action=None, reward=None, next_state=None, done=None, **infos) Description Appends new data to the list ExperienceReplay. Arguments state (tensor/ndarray/list) - Originating state. action (tensor/ndarray/list) - Executed action. reward (tensor/ndarray/list) - Observed reward. next_state (tensor/ndarray/list) - Resulting state. done (tensor/bool) - Is next_state a terminal (absorbing) state ? infos (dict, optional , default=None) - Additional information on the transition. Example replay.append(state, action, reward, next_state, done, info={ 'density': density, 'log_prob': density.log_prob(action), })","title":"append"},{"location":"docs/cherry/#sample","text":"ExperienceReplay.sample(self, size=1, contiguous=False, episodes=False) Samples from the Experience replay. Arguments size (int, optional , default=1) - The number of samples. contiguous (bool, optional , default=False) - Whether to sample contiguous transitions. episodes (bool, optional , default=False) - Sample full episodes, instead of transitions. Return ExperienceReplay - New ExperienceReplay containing the sampled transitions.","title":"sample"},{"location":"docs/cherry/#empty","text":"ExperienceReplay.empty(self) Description Removes all data from an ExperienceReplay. Example replay.empty()","title":"empty"},{"location":"docs/cherry/#totensor","text":"totensor(array, dtype=None) [Source] Description Returns the minimium viewable size of a tensor. e.g. (1, 1, 3, 4) -> (3, 4) References Arguments Returns Example","title":"totensor"},{"location":"docs/cherry/#normalize","text":"normalize(tensor, epsilon=1e-08) [Source] Description Normalizes a tensor to zero mean and unit std. References Arguments Returns Example","title":"normalize"},{"location":"docs/cherry.models/","text":"cherry.models.utils RandomPolicy RandomPolicy(self, env, *args, **kwargs) [Source] Description Policy that randomly samples actions from the environment action space. Arguments env (Environment) - Environment from which to sample actions. Example policy = ch.models.RandomPolicy(env) env = envs.Runner(env) replay = env.run(policy, steps=2048) polyak_average polyak_average(source, target, alpha) [Source] Description Shifts the parameters of source towards those of target. Note: the parameter alpha indicates the convex combination weight of the source. (i.e. the old parameters are kept at a rate of alpha .) References Polyak, B., and A. Juditsky. 1992. \u201cAcceleration of Stochastic Approximation by Averaging.\u201d Arguments source (nn.Module) - The module to be shifted. target (nn.Module) - The module indicating the shift direction. alpha (float) - Strength of the shift. Example target_qf = nn.Linear(23, 34) qf = nn.Linear(23, 34) ch.models.polyak_average(target_qf, qf, alpha=0.9) cherry.models.tabular StateValueFunction StateValueFunction(self, state_size, init=None) [Source] Description Stores a table of state values, V(s), one for each state. Assumes that the states are one-hot encoded. Also, the returned values are differentiable and can be used in conjunction with PyTorch's optimizers. Arguments state_size (int) - The number of states in the environment. init (function, optional , default=None) - The initialization scheme for the values in the table. (Default is 0.) References Sutton, Richard, and Andrew Barto. 2018. Reinforcement Learning, Second Edition. The MIT Press. Example vf = StateValueFunction(env.state_size) state = env.reset() state = ch.onehot(state, env.state_size) state_value = vf(state) ActionValueFunction ActionValueFunction(self, state_size, action_size, init=None) [Source] Description Stores a table of action values, Q(s, a), one for each (state, action) pair. Assumes that the states and actions are one-hot encoded. Also, the returned values are differentiable and can be used in conjunction with PyTorch's optimizers. Arguments state_size (int) - The number of states in the environment. action_size (int) - The number of actions per state. init (function, optional , default=None) - The initialization scheme for the values in the table. (Default is 0.) References Sutton, Richard, and Andrew Barto. 2018. Reinforcement Learning, Second Edition. The MIT Press. Example qf = ActionValueFunction(env.state_size, env.action_size) state = env.reset() state = ch.onehot(state, env.state_size) all_action_values = qf(state) action = ch.onehot(0, env.action_size) action_value = qf(state, action) cherry.models.atari NatureFeatures NatureFeatures(self, input_size=4, hidden_size=512) [Source] Description The convolutional body of the DQN architecture. References Mnih et al. 2015. \u201cHuman-Level Control through Deep Reinforcement Learning.\u201d Mnih et al. 2016. \u201cAsynchronous Methods for Deep Reinforcement Learning.\u201d Credit Adapted from Ilya Kostrikov's implementation. Arguments input_size (int) - Number of channels. (Stacked frames in original implementation.) output_size (int, optional , default=512) - Size of the output representation. NatureActor NatureActor(self, input_size, output_size) [Source] Description The actor head of the A3C architecture. References Mnih et al. 2015. \u201cHuman-Level Control through Deep Reinforcement Learning.\u201d Mnih et al. 2016. \u201cAsynchronous Methods for Deep Reinforcement Learning.\u201d Credit Adapted from Ilya Kostrikov's implementation. Arguments input_size (int) - Size of input of the fully connected layers output_size (int) - Size of the action space. NatureCritic NatureCritic(self, input_size, output_size=1) [Source] Description The critic head of the A3C architecture. References Mnih et al. 2015. \u201cHuman-Level Control through Deep Reinforcement Learning.\u201d Mnih et al. 2016. \u201cAsynchronous Methods for Deep Reinforcement Learning.\u201d Credit Adapted from Ilya Kostrikov's implementation. Arguments input_size (int) - Size of input of the fully connected layers output_size (int, optional , default=1) - Size of the value. cherry.models.control ControlMLP ControlMLP(self, input_size, output_size, layer_sizes=None) [Source] Description A multi-layer perceptron with proper initialization for continuous control. Credit Adapted from Ilya Kostrikov's implementation. Arguments inputs_size (int) - Size of input. output_size (int) - Size of output. layer_sizes (list, optional , default=None) - A list of ints, each indicating the size of a hidden layer. (Defaults to two hidden layers of 64 units.) Example target_qf = ch.models.control.ControlMLP(23, 34, layer_sizes=[32, 32]) Actor Actor(self, input_size, output_size, layer_sizes=None) [Source] Description A multi-layer perceptron with initialization designed for choosing actions in continuous control environments. Credit Adapted from Ilya Kostrikov's implementation. Arguments inputs_size (int) - Size of input. output_size (int) - Size of action size. layer_sizes (list, optional , default=None) - A list of ints, each indicating the size of a hidden layer. (Defaults to two hidden layers of 64 units.) Example policy_mean = ch.models.control.Actor(28, 8, layer_sizes=[64, 32, 16])","title":"cherry.models"},{"location":"docs/cherry.models/#cherrymodelsutils","text":"","title":"cherry.models.utils"},{"location":"docs/cherry.models/#randompolicy","text":"RandomPolicy(self, env, *args, **kwargs) [Source] Description Policy that randomly samples actions from the environment action space. Arguments env (Environment) - Environment from which to sample actions. Example policy = ch.models.RandomPolicy(env) env = envs.Runner(env) replay = env.run(policy, steps=2048)","title":"RandomPolicy"},{"location":"docs/cherry.models/#polyak_average","text":"polyak_average(source, target, alpha) [Source] Description Shifts the parameters of source towards those of target. Note: the parameter alpha indicates the convex combination weight of the source. (i.e. the old parameters are kept at a rate of alpha .) References Polyak, B., and A. Juditsky. 1992. \u201cAcceleration of Stochastic Approximation by Averaging.\u201d Arguments source (nn.Module) - The module to be shifted. target (nn.Module) - The module indicating the shift direction. alpha (float) - Strength of the shift. Example target_qf = nn.Linear(23, 34) qf = nn.Linear(23, 34) ch.models.polyak_average(target_qf, qf, alpha=0.9)","title":"polyak_average"},{"location":"docs/cherry.models/#cherrymodelstabular","text":"","title":"cherry.models.tabular"},{"location":"docs/cherry.models/#statevaluefunction","text":"StateValueFunction(self, state_size, init=None) [Source] Description Stores a table of state values, V(s), one for each state. Assumes that the states are one-hot encoded. Also, the returned values are differentiable and can be used in conjunction with PyTorch's optimizers. Arguments state_size (int) - The number of states in the environment. init (function, optional , default=None) - The initialization scheme for the values in the table. (Default is 0.) References Sutton, Richard, and Andrew Barto. 2018. Reinforcement Learning, Second Edition. The MIT Press. Example vf = StateValueFunction(env.state_size) state = env.reset() state = ch.onehot(state, env.state_size) state_value = vf(state)","title":"StateValueFunction"},{"location":"docs/cherry.models/#actionvaluefunction","text":"ActionValueFunction(self, state_size, action_size, init=None) [Source] Description Stores a table of action values, Q(s, a), one for each (state, action) pair. Assumes that the states and actions are one-hot encoded. Also, the returned values are differentiable and can be used in conjunction with PyTorch's optimizers. Arguments state_size (int) - The number of states in the environment. action_size (int) - The number of actions per state. init (function, optional , default=None) - The initialization scheme for the values in the table. (Default is 0.) References Sutton, Richard, and Andrew Barto. 2018. Reinforcement Learning, Second Edition. The MIT Press. Example qf = ActionValueFunction(env.state_size, env.action_size) state = env.reset() state = ch.onehot(state, env.state_size) all_action_values = qf(state) action = ch.onehot(0, env.action_size) action_value = qf(state, action)","title":"ActionValueFunction"},{"location":"docs/cherry.models/#cherrymodelsatari","text":"","title":"cherry.models.atari"},{"location":"docs/cherry.models/#naturefeatures","text":"NatureFeatures(self, input_size=4, hidden_size=512) [Source] Description The convolutional body of the DQN architecture. References Mnih et al. 2015. \u201cHuman-Level Control through Deep Reinforcement Learning.\u201d Mnih et al. 2016. \u201cAsynchronous Methods for Deep Reinforcement Learning.\u201d Credit Adapted from Ilya Kostrikov's implementation. Arguments input_size (int) - Number of channels. (Stacked frames in original implementation.) output_size (int, optional , default=512) - Size of the output representation.","title":"NatureFeatures"},{"location":"docs/cherry.models/#natureactor","text":"NatureActor(self, input_size, output_size) [Source] Description The actor head of the A3C architecture. References Mnih et al. 2015. \u201cHuman-Level Control through Deep Reinforcement Learning.\u201d Mnih et al. 2016. \u201cAsynchronous Methods for Deep Reinforcement Learning.\u201d Credit Adapted from Ilya Kostrikov's implementation. Arguments input_size (int) - Size of input of the fully connected layers output_size (int) - Size of the action space.","title":"NatureActor"},{"location":"docs/cherry.models/#naturecritic","text":"NatureCritic(self, input_size, output_size=1) [Source] Description The critic head of the A3C architecture. References Mnih et al. 2015. \u201cHuman-Level Control through Deep Reinforcement Learning.\u201d Mnih et al. 2016. \u201cAsynchronous Methods for Deep Reinforcement Learning.\u201d Credit Adapted from Ilya Kostrikov's implementation. Arguments input_size (int) - Size of input of the fully connected layers output_size (int, optional , default=1) - Size of the value.","title":"NatureCritic"},{"location":"docs/cherry.models/#cherrymodelscontrol","text":"","title":"cherry.models.control"},{"location":"docs/cherry.models/#controlmlp","text":"ControlMLP(self, input_size, output_size, layer_sizes=None) [Source] Description A multi-layer perceptron with proper initialization for continuous control. Credit Adapted from Ilya Kostrikov's implementation. Arguments inputs_size (int) - Size of input. output_size (int) - Size of output. layer_sizes (list, optional , default=None) - A list of ints, each indicating the size of a hidden layer. (Defaults to two hidden layers of 64 units.) Example target_qf = ch.models.control.ControlMLP(23, 34, layer_sizes=[32, 32])","title":"ControlMLP"},{"location":"docs/cherry.models/#actor","text":"Actor(self, input_size, output_size, layer_sizes=None) [Source] Description A multi-layer perceptron with initialization designed for choosing actions in continuous control environments. Credit Adapted from Ilya Kostrikov's implementation. Arguments inputs_size (int) - Size of input. output_size (int) - Size of action size. layer_sizes (list, optional , default=None) - A list of ints, each indicating the size of a hidden layer. (Defaults to two hidden layers of 64 units.) Example policy_mean = ch.models.control.Actor(28, 8, layer_sizes=[64, 32, 16])","title":"Actor"},{"location":"docs/cherry.nn.init/","text":"cherry.nn.init pong_control_ pong_control_(module, bias=0.1) [Source] Description Control initialization from Vitchyr Pong's implementations. References Arguments Returns Example kostrikov_control_ kostrikov_control_(module, gain=None) [Source] Description Control initialization from Ilya Kostrikov's implementations. References Arguments Returns Example atari_init_ atari_init_(module, gain=None) [Source] Description Atari initialization from Ilya Kostrikov's implementations. References Arguments Returns Example","title":"cherry.nn.init"},{"location":"docs/cherry.nn.init/#cherrynninit","text":"","title":"cherry.nn.init"},{"location":"docs/cherry.nn.init/#pong_control_","text":"pong_control_(module, bias=0.1) [Source] Description Control initialization from Vitchyr Pong's implementations. References Arguments Returns Example","title":"pong_control_"},{"location":"docs/cherry.nn.init/#kostrikov_control_","text":"kostrikov_control_(module, gain=None) [Source] Description Control initialization from Ilya Kostrikov's implementations. References Arguments Returns Example","title":"kostrikov_control_"},{"location":"docs/cherry.nn.init/#atari_init_","text":"atari_init_(module, gain=None) [Source] Description Atari initialization from Ilya Kostrikov's implementations. References Arguments Returns Example","title":"atari_init_"},{"location":"docs/cherry.nn/","text":"ControlLinear ControlLinear(self, *args, **kwargs) [Source] Description Linear layer for control environments. References Arguments Returns Example","title":"cherry.nn"},{"location":"docs/cherry.nn/#controllinear","text":"ControlLinear(self, *args, **kwargs) [Source] Description Linear layer for control environments. References Arguments Returns Example","title":"ControlLinear"},{"location":"docs/cherry.optim/","text":"cherry.optim Description Optimization utilities for scalable, high-performance reinforcement learning. Distributed Distributed(self, params=<required parameter>, opt=<required parameter>, sync=None) [Source] Description Synchronizes the gradients of a model across replicas. At every step, Distributed averages the gradient across all replicas before calling the wrapped optimizer. The sync parameters determines how frequently the parameters are synchronized between replicas, to minimize numerical divergences. This is done by calling the sync_parameters() method. If sync is None , this never happens except upon initialization of the class. Arguments params (iterable) - Iterable of parameters. opt (Optimizer) - The optimizer to wrap and synchronize. sync (int, optional , default=None) - Parameter synchronization frequency. References Zinkevich et al. 2010. \u201cParallelized Stochastic Gradient Descent.\u201d Example opt = optim.Adam(model.parameters()) opt = Distributed(model.parameters(), opt, sync=1) opt.step() opt.sync_parameters() sync_parameters Distributed.sync_parameters(self, root=0) Description Broadcasts all parameters of root to all other replicas. Arguments root (int, optional , default=0) - Rank of root replica.","title":"cherry.optim"},{"location":"docs/cherry.optim/#cherryoptim","text":"Description Optimization utilities for scalable, high-performance reinforcement learning.","title":"cherry.optim"},{"location":"docs/cherry.optim/#distributed","text":"Distributed(self, params=<required parameter>, opt=<required parameter>, sync=None) [Source] Description Synchronizes the gradients of a model across replicas. At every step, Distributed averages the gradient across all replicas before calling the wrapped optimizer. The sync parameters determines how frequently the parameters are synchronized between replicas, to minimize numerical divergences. This is done by calling the sync_parameters() method. If sync is None , this never happens except upon initialization of the class. Arguments params (iterable) - Iterable of parameters. opt (Optimizer) - The optimizer to wrap and synchronize. sync (int, optional , default=None) - Parameter synchronization frequency. References Zinkevich et al. 2010. \u201cParallelized Stochastic Gradient Descent.\u201d Example opt = optim.Adam(model.parameters()) opt = Distributed(model.parameters(), opt, sync=1) opt.step() opt.sync_parameters()","title":"Distributed"},{"location":"docs/cherry.optim/#sync_parameters","text":"Distributed.sync_parameters(self, root=0) Description Broadcasts all parameters of root to all other replicas. Arguments root (int, optional , default=0) - Rank of root replica.","title":"sync_parameters"},{"location":"docs/cherry.pg/","text":"cherry.pg Description Utilities to implement policy gradient algorithms. generalized_advantage generalized_advantage(gamma, tau, rewards, dones, values, next_value) Description Computes the generalized advantage estimator. (GAE) References Schulman et al. 2015. \u201cHigh-Dimensional Continuous Control Using Generalized Advantage Estimation\u201d https://github.com/joschu/modular_rl/blob/master/modular_rl/core.py#L49 Arguments gamma (float) - Discount factor. tau (float) - Bias-variance trade-off. rewards (tensor) - Tensor of rewards. dones (tensor) - Tensor indicating episode termination. Entry is 1 if the transition led to a terminal (absorbing) state, 0 else. values (tensor) - Values for the states producing the rewards. next_value (tensor) - Value of the state obtained after the transition from the state used to compute the last value in values . Returns tensor - Tensor of advantages. Example mass, next_value = policy(replay[-1].next_state) advantages = generalized_advantage(0.99, 0.95, replay.reward(), replay.value(), replay.done(), next_value)","title":"cherry.pg"},{"location":"docs/cherry.pg/#cherrypg","text":"Description Utilities to implement policy gradient algorithms.","title":"cherry.pg"},{"location":"docs/cherry.pg/#generalized_advantage","text":"generalized_advantage(gamma, tau, rewards, dones, values, next_value) Description Computes the generalized advantage estimator. (GAE) References Schulman et al. 2015. \u201cHigh-Dimensional Continuous Control Using Generalized Advantage Estimation\u201d https://github.com/joschu/modular_rl/blob/master/modular_rl/core.py#L49 Arguments gamma (float) - Discount factor. tau (float) - Bias-variance trade-off. rewards (tensor) - Tensor of rewards. dones (tensor) - Tensor indicating episode termination. Entry is 1 if the transition led to a terminal (absorbing) state, 0 else. values (tensor) - Values for the states producing the rewards. next_value (tensor) - Value of the state obtained after the transition from the state used to compute the last value in values . Returns tensor - Tensor of advantages. Example mass, next_value = policy(replay[-1].next_state) advantages = generalized_advantage(0.99, 0.95, replay.reward(), replay.value(), replay.done(), next_value)","title":"generalized_advantage"},{"location":"docs/cherry.plot/","text":"cherry.plot Description Plotting utilities for reproducible research. ci95 ci95(values) [Source] Description Computes the 95% confidence interval around the given values. Arguments values (list) - List of values for which to compute the 95% confidence interval. Returns (float, float) The lower and upper bounds of the confidence interval. Example from statistics import mean smoothed = [] for replay in replays: rewards = replay.rewards.view(-1).tolist() y_smoothed = ch.plot.smooth(rewards) smoothed.append(y_smoothed) means = [mean(r) for r in zip(*smoothed)] confidences = [ch.plot.ci95(r) for r in zip(*smoothed)] lower_bound = [conf[0] for conf in confidences] upper_bound = [conf[1] for conf in confidences] exponential_smoothing exponential_smoothing(x, y=None, temperature=1.0) [Source] Decription Two-sided exponential moving average for smoothing a curve. It performs regular exponential moving average twice from two different sides and then combines the results together. Arguments x (ndarray/tensor/list) - x values, in accending order. y (ndarray/tensor/list) - y values. temperature (float, optional , default=1.0) - The higher, the smoother. Return ndarray - x values after resampling. ndarray - y values after smoothing. Credit Adapted from OpenAI's baselines implementation. Example from cherry.plot import exponential_smoothing x_smoothed, y_smoothed, _ = exponential_smoothing(x_original, y_original, temperature=3.)","title":"cherry.plot"},{"location":"docs/cherry.plot/#cherryplot","text":"Description Plotting utilities for reproducible research.","title":"cherry.plot"},{"location":"docs/cherry.plot/#ci95","text":"ci95(values) [Source] Description Computes the 95% confidence interval around the given values. Arguments values (list) - List of values for which to compute the 95% confidence interval. Returns (float, float) The lower and upper bounds of the confidence interval. Example from statistics import mean smoothed = [] for replay in replays: rewards = replay.rewards.view(-1).tolist() y_smoothed = ch.plot.smooth(rewards) smoothed.append(y_smoothed) means = [mean(r) for r in zip(*smoothed)] confidences = [ch.plot.ci95(r) for r in zip(*smoothed)] lower_bound = [conf[0] for conf in confidences] upper_bound = [conf[1] for conf in confidences]","title":"ci95"},{"location":"docs/cherry.plot/#exponential_smoothing","text":"exponential_smoothing(x, y=None, temperature=1.0) [Source] Decription Two-sided exponential moving average for smoothing a curve. It performs regular exponential moving average twice from two different sides and then combines the results together. Arguments x (ndarray/tensor/list) - x values, in accending order. y (ndarray/tensor/list) - y values. temperature (float, optional , default=1.0) - The higher, the smoother. Return ndarray - x values after resampling. ndarray - y values after smoothing. Credit Adapted from OpenAI's baselines implementation. Example from cherry.plot import exponential_smoothing x_smoothed, y_smoothed, _ = exponential_smoothing(x_original, y_original, temperature=3.)","title":"exponential_smoothing"},{"location":"docs/cherry.td/","text":"cherry.td Description Utilities to implement temporal difference algorithms. discount discount(gamma, rewards, dones, bootstrap=0.0) Description Discounts rewards at an rate of gamma. References Sutton, Richard, and Andrew Barto. 2018. Reinforcement Learning, Second Edition. The MIT Press. Arguments gamma (float) - Discount factor. rewards (tensor) - Tensor of rewards. dones (tensor) - Tensor indicating episode termination. Entry is 1 if the transition led to a terminal (absorbing) state, 0 else. bootstrap (float, optional , default=0.0) - Bootstrap the last reward with this value. Returns tensor - Tensor of discounted rewards. Example rewards = th.ones(23, 1) * 8 dones = th.zeros_like(rewards) dones[-1] += 1.0 discounted = ch.rl.discount(0.99, rewards, dones, bootstrap=1.0) temporal_difference temporal_difference(gamma, rewards, dones, values, next_values) Description Returns the temporal difference residual. Reference Sutton, Richard S. 1988. \u201cLearning to Predict by the Methods of Temporal Differences.\u201d Machine Learning 3 (1): 9\u201344. Sutton, Richard, and Andrew Barto. 2018. Reinforcement Learning, Second Edition. The MIT Press. Arguments gamma (float) - Discount factor. rewards (tensor) - Tensor of rewards. dones (tensor) - Tensor indicating episode termination. Entry is 1 if the transition led to a terminal (absorbing) state, 0 else. values (tensor) - Values for the states producing the rewards. next_values (tensor) - Values of the state obtained after the transition from the state used to compute the last value in values . Example values = vf(replay.states()) next_values = vf(replay.next_states()) td_errors = temporal_difference(0.99, replay.reward(), replay.done(), values, next_values)","title":"cherry.td"},{"location":"docs/cherry.td/#cherrytd","text":"Description Utilities to implement temporal difference algorithms.","title":"cherry.td"},{"location":"docs/cherry.td/#discount","text":"discount(gamma, rewards, dones, bootstrap=0.0) Description Discounts rewards at an rate of gamma. References Sutton, Richard, and Andrew Barto. 2018. Reinforcement Learning, Second Edition. The MIT Press. Arguments gamma (float) - Discount factor. rewards (tensor) - Tensor of rewards. dones (tensor) - Tensor indicating episode termination. Entry is 1 if the transition led to a terminal (absorbing) state, 0 else. bootstrap (float, optional , default=0.0) - Bootstrap the last reward with this value. Returns tensor - Tensor of discounted rewards. Example rewards = th.ones(23, 1) * 8 dones = th.zeros_like(rewards) dones[-1] += 1.0 discounted = ch.rl.discount(0.99, rewards, dones, bootstrap=1.0)","title":"discount"},{"location":"docs/cherry.td/#temporal_difference","text":"temporal_difference(gamma, rewards, dones, values, next_values) Description Returns the temporal difference residual. Reference Sutton, Richard S. 1988. \u201cLearning to Predict by the Methods of Temporal Differences.\u201d Machine Learning 3 (1): 9\u201344. Sutton, Richard, and Andrew Barto. 2018. Reinforcement Learning, Second Edition. The MIT Press. Arguments gamma (float) - Discount factor. rewards (tensor) - Tensor of rewards. dones (tensor) - Tensor indicating episode termination. Entry is 1 if the transition led to a terminal (absorbing) state, 0 else. values (tensor) - Values for the states producing the rewards. next_values (tensor) - Values of the state obtained after the transition from the state used to compute the last value in values . Example values = vf(replay.states()) next_values = vf(replay.next_states()) td_errors = temporal_difference(0.99, replay.reward(), replay.done(), values, next_values)","title":"temporal_difference"},{"location":"tutorials/debugging_rl/","text":"Debugging Reinforcement Learning Objectives: Show how cherry helps you catch bugs with defensive programming, Show how to use the VisdomLogger and the Record wrapper Provide some tips and tricks on debugging.","title":"Debugging Reinforcement Learning"},{"location":"tutorials/debugging_rl/#debugging-reinforcement-learning","text":"Objectives: Show how cherry helps you catch bugs with defensive programming, Show how to use the VisdomLogger and the Record wrapper Provide some tips and tricks on debugging.","title":"Debugging Reinforcement Learning"},{"location":"tutorials/distributed_ppo/","text":"Distributed Training with PPO Objectives: Show how to use the control models, Show how to setup the distributed optimizer, Showcase the PPO methods. Pretty short too.","title":"Distributed Training with PPO"},{"location":"tutorials/distributed_ppo/#distributed-training-with-ppo","text":"Objectives: Show how to use the control models, Show how to setup the distributed optimizer, Showcase the PPO methods. Pretty short too.","title":"Distributed Training with PPO"},{"location":"tutorials/getting_started/","text":"Getting Started with Cherry This document provides an overview of the philosophy behind cherry, the tools it provides, and a small illustrative example. By the end of this tutorial, you should be well-equiped to incorporate cherry in your research workflow. We assume that you are already familiar with reinforcement learning. If, instead, you're looking for an introduction to the field we recommend looking at Josh Achiam's Spinning Up in Deep RL . Installation The first step in getting started with cherry is to install it. Since it is implemented purely on top of PyTorch, you can easily do that by typing the following command in your favorite shell. pip install cherry-rl By default cherry only has two dependencies: torch and gym . However, more dependencies might be required if you plan to use some more specific functionalities. For example, the OpenAIAtari wrapper requires OpenCV ( pip install opencv-python ) and the VisdomLogger requires visdom ( pip install visdom ). Note While cherry depends on Gym for its environment wrappers, it doesn't restrict you to Gym environments. For instance, check the examples using simple_rl and pycolab environments for Gym-free usage of cherry. Overview Why do we need cherry ? There are many reinforcement learning libraries, many of which feature high-quality implementations. However, few of them provide the kind of low-level utilities useful to researchers. Cherry aims to alleviate this issue. Instead of an interface akin to PPO(env_name).train(1000) , it provides researchers with a set of tools they can use to write readable, replicable, and flexible implementations. Cherry prioritizes time-to-correct-implementation over time-to-run, by explicitely helping you check, debug, and reliably report your results. How to use cherry ? Our goal is to make cherry a natural extension to PyTorch with reinforcement learning in mind. To this end, we closely follow the package structure of PyTorch while providing additional utilities where we saw fit. So if your goal is to implement a novel distributed off-policy policy gradient algorithm, you can count on cherry to provide you experience replays, policy gradient losses, discounting and advantage functions, and distributed optimizers. Those functions not only reduce the time spent writing code, they also check that your implementation is sane. (e.g. do the log probabilities and rewards have identical shapes?) Moreover, cherry provides implementation details -- unfortunately -- necessary to make deep reinforcement learning work. (e.g. initializations, modules, and wrappers commonly used in robotic or Atari benchmarks.) Importantly, it includes built-in debugging functionalities: cherry can help you visualize what is happening under the hood of your algorithm to help you find bugs faster. What's the future of cherry ? Reinforcement learning is a fast moving field, and it is difficult to predict which advances are safe bets for the future. Our long-term development strategy can be summarized as follows. Have as many recent and high-quality examples as possible. Merge advances that turn up to be fundamental in theory or practice into the core library. We hope to combat the reproducibility crisis by extensively testing and benchmarking our implementations. Note Cherry is in its early days and is still missing some of the well-established methods from the past 60 years. Those ones are being implemented as fast as we can :) Core Features The following features are fundamental components of cherry. Transitions and Experience Replay A majority of algorithms needs to store, retrieve, and sample past experience. To do that, you can use cherry's ExperienceReplay . An experience replay is implemented as a wrapper around a standard Python list. The major difference is that the append() method expects arguments used to create a Transition . In addition to behaving like a list, it exposes methods that act on this list, such as to(device) (moves the replay to a device), sample() (randomly samples some experience), or load() / save() (for convenient serialization). An ExperienceReplay contains Transition s, which are akin to ( state , action , reward , next_state , done ) named tuples with possibly additional custom fields. Those fields are easily accessible directly from the replay by accessing the method named after them. For example, calling replay.action() will fetch the action field from every transition stored in replay , stack them along the first dimension, and return that large tensor. The same is true for custom fields; if all transitions have a logprob field, replay.logprob() will return the result of stacking them. Temporal Difference and Policy Gradients Many low-level utilities used to implement temporal difference and policy gradient algorithms are available in the cherry.td and cherry.pg modules, respectively. Those modules include classical methods such as discounting rewards or computing the temporal difference , as well as more recent advances such as the generalized advantage estimator . We tried our best to avoid philosophical dissonance when a method belonged to both families of algorithms, but it is our understanding that conflict is innevitable. Models and PyTorch Similar to PyTorch, we provide differentiable modules in cherry.nn , domain-specific initialization schemes in cherry.nn.init , and optimization utilities in cherry.optim . In addition, popular higher-level models are available in cherry.models ; for instance, those include tabular modules , the Atari CNN features extractor , and a Multi-Layer Perceptron for continuous control. Gym Wrappers Given the popularity of OpenAI Gym environment in modern reinforcement learning benchmarks, cherry includes convenient wrappers in the cherry.envs package. Examples include normalization of states and actions , Atari frames pre-processing, customizable action / reward processing, and automatic collection of experience in a replay. Plots Reporting comparable results has become a central problem in modern reinforcement learning. In order to alleviate this issue, cherry provides utilities to smooth and compute confidence intervals over lists of rewards. Those are available in the cherry.plot submodule. Implementing Policy Gradient The following snippet demonstrates how to use cherry to implement the policy gradient theorem. import cherry as ch env = gym.make('CartPole-v0') env = ch.envs.Logger(env, interval=1000) env = ch.envs.Torch(env) env = ch.envs.Runner(env) env.seed(42) policy = PolicyNet() optimizer = optim.Adam(policy.parameters(), lr=1e-2) action_dist = ch.distributions.ActionDistribution(env) def get_action(state): mass = action_dist(policy(state)) action = mass.sample() log_prob = mass.log_prob(action) return action, {'log_prob': log_prob} for step in range(1000): replay = env.run(get_action, episodes=1) rewards = ch.td.discount(0.99, replay.reward(), replay.done()) rewards = ch.normalize(rewards) loss = -th.sum(replay.log_prob() * rewards) optimizer.zero_grad() loss.backward() optimizer.step() After importing cherry, the first step is to instanciate, wrap, and seed the desired gym environment. env = gym.make('CartPole-v0') env = ch.envs.Logger(env, interval=1000) env = ch.envs.Torch(env) env = ch.envs.Runner(env) env.seed(42) The Logger , Torch , and Runner classes are Gym environment wrappers that systematically modify the behaviour of an environment: Logger keeps track of metrics and prints them at a given interval. Torch converts Gym states into PyTorch tensors, and action tensors into numpy arrays. Runner implements a run method which allows to easily gather transitions for a number of steps or episodes. One particularity of wrappers is that they automatically expose the methods of the wrapped environment if it is not reimplemented: so when calling env.seed(42) we are calling the method from CartPole-v0 . Second, we instanciate the policy, optimizer, as well as the action distribution. The action distribution is created by calling action_dist = ch.distributions.ActionDistribution(env) which will automatically choose between a diagonal Gaussian for continuous action-spaces or a categorical distribution discrete ones. Next, we define get_action which defines how to get an action from our agent and will be used in conjuction to env.run() to quickly collect experience data: replay = env.run(get_action, episodes=1) env.run() assumes that the first returned value by get_action is the action to be passed to the environment and the second, optional, returned value is a dictionary to be saved into the experience replay. Under the hood, env.run() creates a new ExperienceReplay and fills it with the desired number of transitions; instead of episodes=1 we could have passed steps=100 . Finally, we discount and normalize the rewards and take an optimization step on the policy gradient loss. rewards = ch.td.discount(0.99, replay.reward(), replay.done()) rewards = ch.normalize(rewards) loss = -th.sum(replay.log_prob() * rewards) optimizer.zero_grad() loss.backward() optimizer.step() When calling replay.reward() , replay.done() , or replay.log_prob() , the experience replay will concatenate the corresponding attribute across all of its transitions and return a new tensor . This means that this operation is rather expensive (cache it when possible) and that modifying this tensor does not modify the corresponding transitions in replay . Note that in this case log_prob is a custom attribute which is not declared in the original implementation of ExperienceReplay , and we could have given it any name by changing the dictionary key in get_action() . Conclusion You should now be able to use cherry in your own work. For more information, have a look at the documentation , the other tutorials , or the numerous examples . Since one of the characteristics of cherry is to avoid providing \"pre-baked\" algorithms, we tried our best to heavily document its usage.","title":"Getting Started with Cherry"},{"location":"tutorials/getting_started/#getting-started-with-cherry","text":"This document provides an overview of the philosophy behind cherry, the tools it provides, and a small illustrative example. By the end of this tutorial, you should be well-equiped to incorporate cherry in your research workflow. We assume that you are already familiar with reinforcement learning. If, instead, you're looking for an introduction to the field we recommend looking at Josh Achiam's Spinning Up in Deep RL .","title":"Getting Started with Cherry"},{"location":"tutorials/getting_started/#installation","text":"The first step in getting started with cherry is to install it. Since it is implemented purely on top of PyTorch, you can easily do that by typing the following command in your favorite shell. pip install cherry-rl By default cherry only has two dependencies: torch and gym . However, more dependencies might be required if you plan to use some more specific functionalities. For example, the OpenAIAtari wrapper requires OpenCV ( pip install opencv-python ) and the VisdomLogger requires visdom ( pip install visdom ). Note While cherry depends on Gym for its environment wrappers, it doesn't restrict you to Gym environments. For instance, check the examples using simple_rl and pycolab environments for Gym-free usage of cherry.","title":"Installation"},{"location":"tutorials/getting_started/#overview","text":"Why do we need cherry ? There are many reinforcement learning libraries, many of which feature high-quality implementations. However, few of them provide the kind of low-level utilities useful to researchers. Cherry aims to alleviate this issue. Instead of an interface akin to PPO(env_name).train(1000) , it provides researchers with a set of tools they can use to write readable, replicable, and flexible implementations. Cherry prioritizes time-to-correct-implementation over time-to-run, by explicitely helping you check, debug, and reliably report your results. How to use cherry ? Our goal is to make cherry a natural extension to PyTorch with reinforcement learning in mind. To this end, we closely follow the package structure of PyTorch while providing additional utilities where we saw fit. So if your goal is to implement a novel distributed off-policy policy gradient algorithm, you can count on cherry to provide you experience replays, policy gradient losses, discounting and advantage functions, and distributed optimizers. Those functions not only reduce the time spent writing code, they also check that your implementation is sane. (e.g. do the log probabilities and rewards have identical shapes?) Moreover, cherry provides implementation details -- unfortunately -- necessary to make deep reinforcement learning work. (e.g. initializations, modules, and wrappers commonly used in robotic or Atari benchmarks.) Importantly, it includes built-in debugging functionalities: cherry can help you visualize what is happening under the hood of your algorithm to help you find bugs faster. What's the future of cherry ? Reinforcement learning is a fast moving field, and it is difficult to predict which advances are safe bets for the future. Our long-term development strategy can be summarized as follows. Have as many recent and high-quality examples as possible. Merge advances that turn up to be fundamental in theory or practice into the core library. We hope to combat the reproducibility crisis by extensively testing and benchmarking our implementations. Note Cherry is in its early days and is still missing some of the well-established methods from the past 60 years. Those ones are being implemented as fast as we can :)","title":"Overview"},{"location":"tutorials/getting_started/#core-features","text":"The following features are fundamental components of cherry.","title":"Core Features"},{"location":"tutorials/getting_started/#transitions-and-experience-replay","text":"A majority of algorithms needs to store, retrieve, and sample past experience. To do that, you can use cherry's ExperienceReplay . An experience replay is implemented as a wrapper around a standard Python list. The major difference is that the append() method expects arguments used to create a Transition . In addition to behaving like a list, it exposes methods that act on this list, such as to(device) (moves the replay to a device), sample() (randomly samples some experience), or load() / save() (for convenient serialization). An ExperienceReplay contains Transition s, which are akin to ( state , action , reward , next_state , done ) named tuples with possibly additional custom fields. Those fields are easily accessible directly from the replay by accessing the method named after them. For example, calling replay.action() will fetch the action field from every transition stored in replay , stack them along the first dimension, and return that large tensor. The same is true for custom fields; if all transitions have a logprob field, replay.logprob() will return the result of stacking them.","title":"Transitions and Experience Replay"},{"location":"tutorials/getting_started/#temporal-difference-and-policy-gradients","text":"Many low-level utilities used to implement temporal difference and policy gradient algorithms are available in the cherry.td and cherry.pg modules, respectively. Those modules include classical methods such as discounting rewards or computing the temporal difference , as well as more recent advances such as the generalized advantage estimator . We tried our best to avoid philosophical dissonance when a method belonged to both families of algorithms, but it is our understanding that conflict is innevitable.","title":"Temporal Difference and Policy Gradients"},{"location":"tutorials/getting_started/#models-and-pytorch","text":"Similar to PyTorch, we provide differentiable modules in cherry.nn , domain-specific initialization schemes in cherry.nn.init , and optimization utilities in cherry.optim . In addition, popular higher-level models are available in cherry.models ; for instance, those include tabular modules , the Atari CNN features extractor , and a Multi-Layer Perceptron for continuous control.","title":"Models and PyTorch"},{"location":"tutorials/getting_started/#gym-wrappers","text":"Given the popularity of OpenAI Gym environment in modern reinforcement learning benchmarks, cherry includes convenient wrappers in the cherry.envs package. Examples include normalization of states and actions , Atari frames pre-processing, customizable action / reward processing, and automatic collection of experience in a replay.","title":"Gym Wrappers"},{"location":"tutorials/getting_started/#plots","text":"Reporting comparable results has become a central problem in modern reinforcement learning. In order to alleviate this issue, cherry provides utilities to smooth and compute confidence intervals over lists of rewards. Those are available in the cherry.plot submodule.","title":"Plots"},{"location":"tutorials/getting_started/#implementing-policy-gradient","text":"The following snippet demonstrates how to use cherry to implement the policy gradient theorem. import cherry as ch env = gym.make('CartPole-v0') env = ch.envs.Logger(env, interval=1000) env = ch.envs.Torch(env) env = ch.envs.Runner(env) env.seed(42) policy = PolicyNet() optimizer = optim.Adam(policy.parameters(), lr=1e-2) action_dist = ch.distributions.ActionDistribution(env) def get_action(state): mass = action_dist(policy(state)) action = mass.sample() log_prob = mass.log_prob(action) return action, {'log_prob': log_prob} for step in range(1000): replay = env.run(get_action, episodes=1) rewards = ch.td.discount(0.99, replay.reward(), replay.done()) rewards = ch.normalize(rewards) loss = -th.sum(replay.log_prob() * rewards) optimizer.zero_grad() loss.backward() optimizer.step() After importing cherry, the first step is to instanciate, wrap, and seed the desired gym environment. env = gym.make('CartPole-v0') env = ch.envs.Logger(env, interval=1000) env = ch.envs.Torch(env) env = ch.envs.Runner(env) env.seed(42) The Logger , Torch , and Runner classes are Gym environment wrappers that systematically modify the behaviour of an environment: Logger keeps track of metrics and prints them at a given interval. Torch converts Gym states into PyTorch tensors, and action tensors into numpy arrays. Runner implements a run method which allows to easily gather transitions for a number of steps or episodes. One particularity of wrappers is that they automatically expose the methods of the wrapped environment if it is not reimplemented: so when calling env.seed(42) we are calling the method from CartPole-v0 . Second, we instanciate the policy, optimizer, as well as the action distribution. The action distribution is created by calling action_dist = ch.distributions.ActionDistribution(env) which will automatically choose between a diagonal Gaussian for continuous action-spaces or a categorical distribution discrete ones. Next, we define get_action which defines how to get an action from our agent and will be used in conjuction to env.run() to quickly collect experience data: replay = env.run(get_action, episodes=1) env.run() assumes that the first returned value by get_action is the action to be passed to the environment and the second, optional, returned value is a dictionary to be saved into the experience replay. Under the hood, env.run() creates a new ExperienceReplay and fills it with the desired number of transitions; instead of episodes=1 we could have passed steps=100 . Finally, we discount and normalize the rewards and take an optimization step on the policy gradient loss. rewards = ch.td.discount(0.99, replay.reward(), replay.done()) rewards = ch.normalize(rewards) loss = -th.sum(replay.log_prob() * rewards) optimizer.zero_grad() loss.backward() optimizer.step() When calling replay.reward() , replay.done() , or replay.log_prob() , the experience replay will concatenate the corresponding attribute across all of its transitions and return a new tensor . This means that this operation is rather expensive (cache it when possible) and that modifying this tensor does not modify the corresponding transitions in replay . Note that in this case log_prob is a custom attribute which is not declared in the original implementation of ExperienceReplay , and we could have given it any name by changing the dictionary key in get_action() .","title":"Implementing Policy Gradient"},{"location":"tutorials/getting_started/#conclusion","text":"You should now be able to use cherry in your own work. For more information, have a look at the documentation , the other tutorials , or the numerous examples . Since one of the characteristics of cherry is to avoid providing \"pre-baked\" algorithms, we tried our best to heavily document its usage.","title":"Conclusion"},{"location":"tutorials/recurrent_a2c/","text":"Recurrent Policy Gradients with A2C Objectives: Show how to use Atari models + wrappers. Show how to implement recurrent policy gradients with cherry. That's it, very short.","title":"Recurrent Policy Gradients with A2C"},{"location":"tutorials/recurrent_a2c/#recurrent-policy-gradients-with-a2c","text":"Objectives: Show how to use Atari models + wrappers. Show how to implement recurrent policy gradients with cherry. That's it, very short.","title":"Recurrent Policy Gradients with A2C"}]}