{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Cherry is reinforcement learning framework for researchers built on top of PyTorch. Unlike other reinforcement learning implementations, cherry doesn't try to provide a single interface to existing algorithms. Instead, it provides you with common tools to write your own algorithms. Drawing from the UNIX philosophy, each tool strives to be as independent from the rest of the framework as possible. So if you don't like a specific tool, you can still use parts of cherry without headaches. Installation For now, cherry is still in development. Clone the repo: git clone https://github.com/seba-1511/cherry cd cherry pip install -e . Upon our first public release, you'll be able to pip install cherry-rl Development Guidelines The master branch is always working, considered stable. The dev branch should always work and is ahead of master , considered cutting edge. To implement a new functionality: branch dev into your_name/functionality_name , implement your functionality, then pull request to dev . It will be periodically merged into master . Usage The following snippet demonstrates some of the tools offered by cherry. import cherry as ch # Wrapping environments env = ch.envs.Logger(env, interval=1000) # Prints rollouts statistics env = ch.envs.Normalized(env, normalize_state=True, normalize_reward=False) env = ch.envs.Torch(env) # Converts actions/states to tensors # Storing and retrieving experience replay = ch.ExperienceReplay() replay.add(old_state, action, reward, state, done, info = { 'log_prob': mass.log_prob(action), # Can add any variable/tensor to the transitions 'value': value }) replay.actions # Tensor of all stored actions replay.states # Tensor of all stored states replay.empty() # Removes all stored experience # Discounting and normalizing rewards rewards = ch.rewards.discount_rewards(GAMMA, replay.rewards, replay.dones) rewards = ch.utils.normalize(th.tensor(rewards)) # Sampling rollouts per episode or samples num_samples, num_episodes = ch.rollouts.collect(env, get_action, replay, num_episodes=10, # alternatively: num_samples=1000, ) Concrete examples are available in the examples/ folder. Documentation The documentation will be written as we begin to converge the core concepts of cherry. TODO Some functionalities that we might want to implement. parallelize environments and a way to handle it with ExperienceReplay , VisdomLogger as a dashboard to debug an implementation, example with reccurent net, minimal but complete documentation, a few extensive tutorials (Getting started with distributed A2C, Advanced usage (which?) with PPO, and another on debugging your algorithms). Acknowledgements Cherry draws inspiration from many reinforcement learning implementations, including OpenAI Baselines , John Schulman's implementations Ilya Kostrikov's implementations , Shangtong Zhang's implementations , RLLab , RLKit .","title":"Home"},{"location":"#installation","text":"For now, cherry is still in development. Clone the repo: git clone https://github.com/seba-1511/cherry cd cherry pip install -e . Upon our first public release, you'll be able to pip install cherry-rl","title":"Installation"},{"location":"#development-guidelines","text":"The master branch is always working, considered stable. The dev branch should always work and is ahead of master , considered cutting edge. To implement a new functionality: branch dev into your_name/functionality_name , implement your functionality, then pull request to dev . It will be periodically merged into master .","title":"Development Guidelines"},{"location":"#usage","text":"The following snippet demonstrates some of the tools offered by cherry. import cherry as ch # Wrapping environments env = ch.envs.Logger(env, interval=1000) # Prints rollouts statistics env = ch.envs.Normalized(env, normalize_state=True, normalize_reward=False) env = ch.envs.Torch(env) # Converts actions/states to tensors # Storing and retrieving experience replay = ch.ExperienceReplay() replay.add(old_state, action, reward, state, done, info = { 'log_prob': mass.log_prob(action), # Can add any variable/tensor to the transitions 'value': value }) replay.actions # Tensor of all stored actions replay.states # Tensor of all stored states replay.empty() # Removes all stored experience # Discounting and normalizing rewards rewards = ch.rewards.discount_rewards(GAMMA, replay.rewards, replay.dones) rewards = ch.utils.normalize(th.tensor(rewards)) # Sampling rollouts per episode or samples num_samples, num_episodes = ch.rollouts.collect(env, get_action, replay, num_episodes=10, # alternatively: num_samples=1000, ) Concrete examples are available in the examples/ folder.","title":"Usage"},{"location":"#documentation","text":"The documentation will be written as we begin to converge the core concepts of cherry.","title":"Documentation"},{"location":"#todo","text":"Some functionalities that we might want to implement. parallelize environments and a way to handle it with ExperienceReplay , VisdomLogger as a dashboard to debug an implementation, example with reccurent net, minimal but complete documentation, a few extensive tutorials (Getting started with distributed A2C, Advanced usage (which?) with PPO, and another on debugging your algorithms).","title":"TODO"},{"location":"#acknowledgements","text":"Cherry draws inspiration from many reinforcement learning implementations, including OpenAI Baselines , John Schulman's implementations Ilya Kostrikov's implementations , Shangtong Zhang's implementations , RLLab , RLKit .","title":"Acknowledgements"},{"location":"docs/cherry.algorithms/","text":"cherry.algorithms.a2c cherry.algorithms.ppo cherry.algorithms.sac policy_loss policy_loss(log_probs, q_actions, alpha=1.0) Arguments: log_probs: the log density of the actions from the current policy on some states. q_actions: the Q-values for those same actions. alpha: the weight of the policy entropy. q_loss q_loss(q_old_pred, v_next, rewards, dones, gamma) Arguments: q_old_pred: Q values on an existing transition. v_next: V values for the resulting state. rewards: observed rewards during the transition. dones: which states were terminal. gamma: discount factor. v_loss v_loss(v_pred, log_probs, q_values, alpha=1.0) Arguments: v_pred: the V values of states from a batch. log_probs: the log density of actions from the current policy on those states. q_values: the Q values of the those actions on those states. alpha: the weight of the policy entropy.","title":"cherry.algorithms"},{"location":"docs/cherry.distributions/","text":"cherry.distributions TanhNormal TanhNormal(self, normal_mean, normal_std) Adapted from Vitchyr Pong's RLkit: https://github.com/vitchyr/rlkit/blob/master/rlkit/torch/distributions.py","title":"cherry.distributions"},{"location":"docs/cherry.envs/","text":"Logger Logger(self, env, interval=1000, episode_interval=10) Tracks and prints some common statistics about the environment. Runner Runner(self, env) OpenAINormalize OpenAINormalize(self, env, state=True, ret=True, clipob=10.0, cliprew=10.0, gamma=0.99, eps=1e-08)","title":"cherry.envs"},{"location":"docs/cherry.experience-replay/","text":"ExperienceReplay ExperienceReplay(self, states=None, actions=None, rewards=None, next_states=None, dones=None, infos=None)","title":"cherry.ExperienceReplay"},{"location":"docs/cherry/","text":"cherry","title":"cherry"},{"location":"docs/cherry.optim/","text":"cherry.optim","title":"cherry.optim"},{"location":"docs/cherry.rewards/","text":"cherry.rewards","title":"cherry.rewards"},{"location":"docs/cherry.utils/","text":"cherry.utils","title":"cherry.utils"},{"location":"docs/experience-replay/","text":"ExperienceReplay ExperienceReplay(self, states=None, actions=None, rewards=None, next_states=None, dones=None, infos=None)","title":"Experience replay"},{"location":"tutorials/continuous_ppo/","text":"","title":"Continuous ppo"},{"location":"tutorials/distributed_a2c/","text":"Tutorial on Distributed A2C","title":"Tutorial on Distributed A2C"},{"location":"tutorials/distributed_a2c/#tutorial-on-distributed-a2c","text":"","title":"Tutorial on Distributed A2C"}]}