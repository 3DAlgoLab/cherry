{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Cherry is reinforcement learning framework for researchers built on top of PyTorch. Unlike other reinforcement learning implementations, cherry doesn't try to provide a single interface to existing algorithms. Instead, it provides you with common tools to write your own algorithms. Drawing from the UNIX philosophy, each tool strives to be as independent from the rest of the framework as possible. So if you don't like a specific tool, you can still use parts of cherry without headaches. Features Pythonic and modular interface \u00e0 la Pytorch, Support for tabular (!) and function approximation algorithms, Various OpenAI Gym environment wrappers, Helper functions for popular algorithms (e.g. A2C, DDPG, TRPO, PPO, SAC), Logging, visualization, and debugging tools, Painless and efficient distributed training on CPUs and GPUs. Installation For now, cherry is still in development. Clone the repo: git clone https://github.com/seba-1511/cherry cd cherry pip install -e . Upon our first public release, you'll be able to pip install cherry-rl Development Guidelines The master branch is always working, considered stable. The dev branch should always work and is ahead of master , considered cutting edge. To implement a new functionality: branch dev into your_name/functionality_name , implement your functionality, then pull request to dev . It will be periodically merged into master . Usage The following snippet demonstrates some of the tools offered by cherry. import cherry as ch # Wrapping environments env = ch.envs.Logger(env, interval=1000) # Prints rollouts statistics env = ch.envs.Normalized(env, normalize_state=True, normalize_reward=False) env = ch.envs.Torch(env) # Converts actions/states to tensors # Storing and retrieving experience replay = ch.ExperienceReplay() replay.add(old_state, action, reward, state, done, info = { 'log_prob': mass.log_prob(action), # Can add any variable/tensor to the transitions 'value': value }) replay.actions # Tensor of all stored actions replay.states # Tensor of all stored states replay.empty() # Removes all stored experience # Discounting and normalizing rewards rewards = ch.rewards.discount_rewards(GAMMA, replay.rewards, replay.dones) rewards = ch.utils.normalize(th.tensor(rewards)) # Sampling rollouts per episode or samples num_samples, num_episodes = ch.rollouts.collect(env, get_action, replay, num_episodes=10, # alternatively: num_samples=1000, ) Concrete examples are available in the examples/ folder. Documentation The documentation will be written as we begin to converge the core concepts of cherry. TODO Some functionalities that we might want to implement. parallelize environments and a way to handle it with ExperienceReplay , VisdomLogger as a dashboard to debug an implementation, example with reccurent net, minimal but complete documentation, a few extensive tutorials (Getting started with distributed A2C, Advanced usage (which?) with PPO, and another on debugging your algorithms). Acknowledgements Cherry draws inspiration from many reinforcement learning implementations, including OpenAI Baselines , John Schulman's implementations Ilya Kostrikov's implementations , Shangtong Zhang's implementations , RLLab , RLKit .","title":"Home"},{"location":"#installation","text":"For now, cherry is still in development. Clone the repo: git clone https://github.com/seba-1511/cherry cd cherry pip install -e . Upon our first public release, you'll be able to pip install cherry-rl","title":"Installation"},{"location":"#development-guidelines","text":"The master branch is always working, considered stable. The dev branch should always work and is ahead of master , considered cutting edge. To implement a new functionality: branch dev into your_name/functionality_name , implement your functionality, then pull request to dev . It will be periodically merged into master .","title":"Development Guidelines"},{"location":"#usage","text":"The following snippet demonstrates some of the tools offered by cherry. import cherry as ch # Wrapping environments env = ch.envs.Logger(env, interval=1000) # Prints rollouts statistics env = ch.envs.Normalized(env, normalize_state=True, normalize_reward=False) env = ch.envs.Torch(env) # Converts actions/states to tensors # Storing and retrieving experience replay = ch.ExperienceReplay() replay.add(old_state, action, reward, state, done, info = { 'log_prob': mass.log_prob(action), # Can add any variable/tensor to the transitions 'value': value }) replay.actions # Tensor of all stored actions replay.states # Tensor of all stored states replay.empty() # Removes all stored experience # Discounting and normalizing rewards rewards = ch.rewards.discount_rewards(GAMMA, replay.rewards, replay.dones) rewards = ch.utils.normalize(th.tensor(rewards)) # Sampling rollouts per episode or samples num_samples, num_episodes = ch.rollouts.collect(env, get_action, replay, num_episodes=10, # alternatively: num_samples=1000, ) Concrete examples are available in the examples/ folder.","title":"Usage"},{"location":"#documentation","text":"The documentation will be written as we begin to converge the core concepts of cherry.","title":"Documentation"},{"location":"#todo","text":"Some functionalities that we might want to implement. parallelize environments and a way to handle it with ExperienceReplay , VisdomLogger as a dashboard to debug an implementation, example with reccurent net, minimal but complete documentation, a few extensive tutorials (Getting started with distributed A2C, Advanced usage (which?) with PPO, and another on debugging your algorithms).","title":"TODO"},{"location":"#acknowledgements","text":"Cherry draws inspiration from many reinforcement learning implementations, including OpenAI Baselines , John Schulman's implementations Ilya Kostrikov's implementations , Shangtong Zhang's implementations , RLLab , RLKit .","title":"Acknowledgements"},{"location":"docs/cherry.algorithms/","text":"cherry.algorithms.a2c policy_loss policy_loss(log_probs, advantages) [Source] Description Advantage Actor-Critic policy loss. References Arguments Returns Example advantages = replay.advantages log_probs = replay.log_probs loss = a2c.policy_loss(log_probs, advantages) state_value_loss state_value_loss(values, rewards) Advantage Actor-Critic value loss. cherry.algorithms.ppo policy_loss policy_loss(new_log_probs, old_log_probs, advantages, clip=0.1) Clipped policy loss. Arguments: new_log_probs: (tensor) old_log_probs: (tensor) advantages: (tensor) clip: (tensor) state_value_loss state_value_loss(new_values, old_values, rewards, clip=0.1) Clipped value loss. Arguments: new_values: (tensor) old_values: (tensor) rewards: (tensor) clip: (tensor) cherry.algorithms.sac policy_loss policy_loss(log_probs, q_actions, alpha=1.0) Arguments: log_probs: the log density of the actions from the current policy on some states. q_actions: the Q-values for those same actions. alpha: the weight of the policy entropy. action_value_loss action_value_loss(q_old_pred, v_next, rewards, dones, gamma) Arguments: q_old_pred: Q values on an existing transition. v_next: V values for the resulting state. rewards: observed rewards during the transition. dones: which states were terminal. gamma: discount factor. state_value_loss state_value_loss(v_pred, log_probs, q_values, alpha=1.0) Arguments: v_pred: the V values of states from a batch. log_probs: the log density of actions from the current policy on those states. q_values: the Q values of the those actions on those states. alpha: the weight of the policy entropy.","title":"cherry.algorithms"},{"location":"docs/cherry.algorithms/#cherryalgorithmsa2c","text":"","title":"cherry.algorithms.a2c"},{"location":"docs/cherry.algorithms/#policy_loss","text":"policy_loss(log_probs, advantages) [Source] Description Advantage Actor-Critic policy loss. References Arguments Returns Example advantages = replay.advantages log_probs = replay.log_probs loss = a2c.policy_loss(log_probs, advantages)","title":"policy_loss"},{"location":"docs/cherry.algorithms/#state_value_loss","text":"state_value_loss(values, rewards) Advantage Actor-Critic value loss.","title":"state_value_loss"},{"location":"docs/cherry.algorithms/#cherryalgorithmsppo","text":"","title":"cherry.algorithms.ppo"},{"location":"docs/cherry.algorithms/#policy_loss_1","text":"policy_loss(new_log_probs, old_log_probs, advantages, clip=0.1) Clipped policy loss. Arguments: new_log_probs: (tensor) old_log_probs: (tensor) advantages: (tensor) clip: (tensor)","title":"policy_loss"},{"location":"docs/cherry.algorithms/#state_value_loss_1","text":"state_value_loss(new_values, old_values, rewards, clip=0.1) Clipped value loss. Arguments: new_values: (tensor) old_values: (tensor) rewards: (tensor) clip: (tensor)","title":"state_value_loss"},{"location":"docs/cherry.algorithms/#cherryalgorithmssac","text":"","title":"cherry.algorithms.sac"},{"location":"docs/cherry.algorithms/#policy_loss_2","text":"policy_loss(log_probs, q_actions, alpha=1.0) Arguments: log_probs: the log density of the actions from the current policy on some states. q_actions: the Q-values for those same actions. alpha: the weight of the policy entropy.","title":"policy_loss"},{"location":"docs/cherry.algorithms/#action_value_loss","text":"action_value_loss(q_old_pred, v_next, rewards, dones, gamma) Arguments: q_old_pred: Q values on an existing transition. v_next: V values for the resulting state. rewards: observed rewards during the transition. dones: which states were terminal. gamma: discount factor.","title":"action_value_loss"},{"location":"docs/cherry.algorithms/#state_value_loss_2","text":"state_value_loss(v_pred, log_probs, q_values, alpha=1.0) Arguments: v_pred: the V values of states from a batch. log_probs: the log density of actions from the current policy on those states. q_values: the Q values of the those actions on those states. alpha: the weight of the policy entropy.","title":"state_value_loss"},{"location":"docs/cherry.distributions/","text":"cherry.distributions Reparameterization Reparameterization(self, density) Reparameterized distribution. ActionDistribution ActionDistribution(self, env, logstd=None, use_probs=False, reparam=False) A helper module to automatically choose the proper policy distribution, based on the environment action_space. TanhNormal TanhNormal(self, normal_mean, normal_std) Adapted from Vitchyr Pong's RLkit: https://github.com/vitchyr/rlkit/blob/master/rlkit/torch/distributions.py","title":"cherry.distributions"},{"location":"docs/cherry.distributions/#cherrydistributions","text":"","title":"cherry.distributions"},{"location":"docs/cherry.distributions/#reparameterization","text":"Reparameterization(self, density) Reparameterized distribution.","title":"Reparameterization"},{"location":"docs/cherry.distributions/#actiondistribution","text":"ActionDistribution(self, env, logstd=None, use_probs=False, reparam=False) A helper module to automatically choose the proper policy distribution, based on the environment action_space.","title":"ActionDistribution"},{"location":"docs/cherry.distributions/#tanhnormal","text":"TanhNormal(self, normal_mean, normal_std) Adapted from Vitchyr Pong's RLkit: https://github.com/vitchyr/rlkit/blob/master/rlkit/torch/distributions.py","title":"TanhNormal"},{"location":"docs/cherry.envs/","text":"cherry.envs.utils flatten_state flatten_state(space, state) get_space_dimension get_space_dimension(space) Wrapper Wrapper(self, env) This class allows to chain Environment Wrappers while still being able to access the properties of wrapped wrappers. Example: env = gym.make('MyEnv-v0') env = envs.Logger(env) env = envs.Runner(env) env.log('asdf', 23) # Uses log() method from envs.Logger. Runner Runner(self, env) Runner wrapper. run Runner.run(self, get_action, steps=None, episodes=None, render=False) Runner wrapper's run method. Logger Logger(self, env, interval=1000, episode_interval=10) Tracks and prints some common statistics about the environment. VisdomLogger VisdomLogger(self, env, interval=1000, episode_interval=100, name=None) Enables logging and debug values to Visdom. Arguments env: The environment to wrap. interval: (int) Update frequency for episodes. Torch Torch(self, env) This wrapper converts * actions from Tensors to numpy, * states from lists/numpy to Tensors. Example: action = Categorical(Tensor([1, 2, 3])).sample() env.step(action) Normalizer Normalizer(self, env, states=True, rewards=True, clip_states=10.0, clip_rewards=10.0, gamma=0.99, eps=1e-08) [Source] Description Normalizes the states and rewards with a running average. Arguments env (Environment) - Environment to normalize. states (bool, optional , default=True) - Whether to normalize the states. rewards (bool, optional , default=True) - Whether to normalize the rewards. clip_states (bool, optional , default=10.0) - Clip each state dimension between [-clip_states, clip_states]. clip_rewards (float, optional , default=10.0) - Clip rewards between [-clip_rewards, clip_rewards]. gamma (float, optional , default=0.99) - Discount factor for rewards running averages. eps (float, optional , default=1e-8) - Numerical stability. Credit Adapted from OpenAI's baselines implementation. Example env = gym.make('CartPole-v0') env = cherry.envs.Normalizer(env, states=True, rewards=False) RewardClipper RewardClipper(self, env) reward RewardClipper.reward(self, reward) Bin reward to {+1, 0, -1} by its sign. Monitor Monitor(self, env, directory, *args, **kwargs) Sugar coating on top of Gym's Monitor. OpenAIAtari OpenAIAtari(self, env) AddTimestep AddTimestep(self, env=None) Adds a timestep information to the state input. Modified from Ilya Kostrikov's implementation: https://github.com/ikostrikov/pytorch-a2c-ppo-acktr/ StateLambda StateLambda(self, env, fn) ActionSpaceScaler ActionSpaceScaler(self, env, clip=1.0) Scales the action space to be in the range (-clip, clip). Adapted from Vitchyr Pong's RLkit: https://github.com/vitchyr/rlkit/blob/master/rlkit/envs/wrappers.py#L41","title":"cherry.envs"},{"location":"docs/cherry.envs/#cherryenvsutils","text":"","title":"cherry.envs.utils"},{"location":"docs/cherry.envs/#flatten_state","text":"flatten_state(space, state)","title":"flatten_state"},{"location":"docs/cherry.envs/#get_space_dimension","text":"get_space_dimension(space)","title":"get_space_dimension"},{"location":"docs/cherry.envs/#wrapper","text":"Wrapper(self, env) This class allows to chain Environment Wrappers while still being able to access the properties of wrapped wrappers. Example: env = gym.make('MyEnv-v0') env = envs.Logger(env) env = envs.Runner(env) env.log('asdf', 23) # Uses log() method from envs.Logger.","title":"Wrapper"},{"location":"docs/cherry.envs/#runner","text":"Runner(self, env) Runner wrapper.","title":"Runner"},{"location":"docs/cherry.envs/#run","text":"Runner.run(self, get_action, steps=None, episodes=None, render=False) Runner wrapper's run method.","title":"run"},{"location":"docs/cherry.envs/#logger","text":"Logger(self, env, interval=1000, episode_interval=10) Tracks and prints some common statistics about the environment.","title":"Logger"},{"location":"docs/cherry.envs/#visdomlogger","text":"VisdomLogger(self, env, interval=1000, episode_interval=100, name=None) Enables logging and debug values to Visdom. Arguments env: The environment to wrap. interval: (int) Update frequency for episodes.","title":"VisdomLogger"},{"location":"docs/cherry.envs/#torch","text":"Torch(self, env) This wrapper converts * actions from Tensors to numpy, * states from lists/numpy to Tensors. Example: action = Categorical(Tensor([1, 2, 3])).sample() env.step(action)","title":"Torch"},{"location":"docs/cherry.envs/#normalizer","text":"Normalizer(self, env, states=True, rewards=True, clip_states=10.0, clip_rewards=10.0, gamma=0.99, eps=1e-08) [Source] Description Normalizes the states and rewards with a running average. Arguments env (Environment) - Environment to normalize. states (bool, optional , default=True) - Whether to normalize the states. rewards (bool, optional , default=True) - Whether to normalize the rewards. clip_states (bool, optional , default=10.0) - Clip each state dimension between [-clip_states, clip_states]. clip_rewards (float, optional , default=10.0) - Clip rewards between [-clip_rewards, clip_rewards]. gamma (float, optional , default=0.99) - Discount factor for rewards running averages. eps (float, optional , default=1e-8) - Numerical stability. Credit Adapted from OpenAI's baselines implementation. Example env = gym.make('CartPole-v0') env = cherry.envs.Normalizer(env, states=True, rewards=False)","title":"Normalizer"},{"location":"docs/cherry.envs/#rewardclipper","text":"RewardClipper(self, env)","title":"RewardClipper"},{"location":"docs/cherry.envs/#reward","text":"RewardClipper.reward(self, reward) Bin reward to {+1, 0, -1} by its sign.","title":"reward"},{"location":"docs/cherry.envs/#monitor","text":"Monitor(self, env, directory, *args, **kwargs) Sugar coating on top of Gym's Monitor.","title":"Monitor"},{"location":"docs/cherry.envs/#openaiatari","text":"OpenAIAtari(self, env)","title":"OpenAIAtari"},{"location":"docs/cherry.envs/#addtimestep","text":"AddTimestep(self, env=None) Adds a timestep information to the state input. Modified from Ilya Kostrikov's implementation: https://github.com/ikostrikov/pytorch-a2c-ppo-acktr/","title":"AddTimestep"},{"location":"docs/cherry.envs/#statelambda","text":"StateLambda(self, env, fn)","title":"StateLambda"},{"location":"docs/cherry.envs/#actionspacescaler","text":"ActionSpaceScaler(self, env, clip=1.0) Scales the action space to be in the range (-clip, clip). Adapted from Vitchyr Pong's RLkit: https://github.com/vitchyr/rlkit/blob/master/rlkit/envs/wrappers.py#L41","title":"ActionSpaceScaler"},{"location":"docs/cherry.experience-replay/","text":"Transition Transition(self, **kwargs) Description Represents a (s, a, r, s', d) tuple. Arguments None Example for transition in replay: print(transition.state) ExperienceReplay ExperienceReplay(self, states=None, actions=None, rewards=None, next_states=None, dones=None, infos=None) [Source] Description Experience replay buffer to store, retrieve, and sample past transitions. ExperienceReplay behaves like a list of transitions, . It also support accessing specific properties, such as states, actions, rewards, next_states, and informations. The first four are returned as tensors, while infos is returned as a list of dicts. The properties of infos can be accessed directly by appending an s to their dictionary key -- see Examples below. In this case, if the values of the infos are tensors, they will be returned as a concatenated Tensor. Otherwise, they default to a list of values. Arguments states (Tensor, optional , default=None) - Tensor of states. actions (Tensor, optional , default=None) - Tensor of actions. rewards (Tensor, optional , default=None) - Tensor of rewards. next_states (Tensor, optional , default=None) - Tensor of next_states. dones (Tensor, optional , default=None) - Tensor of dones. infos (list, optional , default=None) - List of infos. References Lin, Long-Ji. 1992. \u201cSelf-Improving Reactive Agents Based on Reinforcement Learning, Planning and Teaching.\u201d Machine Learning 8 (3): 293\u2013321. Example replay = ch.ExperienceReplay() # Instanciate a new replay replay.add(state, # Add experience to the replay action, reward, next_state, done, info={ 'density': action_density, 'log_prob': action_density.log_prob(action), }) replay.state # Tensor of states replay.actions # Tensor of actions replay.densitys # list of action_density replay.log_probs # Tensor of log_probabilities new_replay = replay[-10:] # Last 10 transitions in new_replay #Sample some previous experience batch = replay.sample(32, contiguous=True) sample ExperienceReplay.sample(self, size=1, contiguous=False, episodes=False) Samples from the Experience replay. Arguments size (int, optional , default=1) - The number of samples. contiguous (bool, optional , default=False) - Whether to sample contiguous transitions. episodes (bool, optional , default=False) - Sample full episodes, instead of transitions. Return ExperienceReplay() containing the sampled transitions.","title":"cherry.ExperienceReplay"},{"location":"docs/cherry.experience-replay/#transition","text":"Transition(self, **kwargs) Description Represents a (s, a, r, s', d) tuple. Arguments None Example for transition in replay: print(transition.state)","title":"Transition"},{"location":"docs/cherry.experience-replay/#experiencereplay","text":"ExperienceReplay(self, states=None, actions=None, rewards=None, next_states=None, dones=None, infos=None) [Source] Description Experience replay buffer to store, retrieve, and sample past transitions. ExperienceReplay behaves like a list of transitions, . It also support accessing specific properties, such as states, actions, rewards, next_states, and informations. The first four are returned as tensors, while infos is returned as a list of dicts. The properties of infos can be accessed directly by appending an s to their dictionary key -- see Examples below. In this case, if the values of the infos are tensors, they will be returned as a concatenated Tensor. Otherwise, they default to a list of values. Arguments states (Tensor, optional , default=None) - Tensor of states. actions (Tensor, optional , default=None) - Tensor of actions. rewards (Tensor, optional , default=None) - Tensor of rewards. next_states (Tensor, optional , default=None) - Tensor of next_states. dones (Tensor, optional , default=None) - Tensor of dones. infos (list, optional , default=None) - List of infos. References Lin, Long-Ji. 1992. \u201cSelf-Improving Reactive Agents Based on Reinforcement Learning, Planning and Teaching.\u201d Machine Learning 8 (3): 293\u2013321. Example replay = ch.ExperienceReplay() # Instanciate a new replay replay.add(state, # Add experience to the replay action, reward, next_state, done, info={ 'density': action_density, 'log_prob': action_density.log_prob(action), }) replay.state # Tensor of states replay.actions # Tensor of actions replay.densitys # list of action_density replay.log_probs # Tensor of log_probabilities new_replay = replay[-10:] # Last 10 transitions in new_replay #Sample some previous experience batch = replay.sample(32, contiguous=True)","title":"ExperienceReplay"},{"location":"docs/cherry.experience-replay/#sample","text":"ExperienceReplay.sample(self, size=1, contiguous=False, episodes=False) Samples from the Experience replay. Arguments size (int, optional , default=1) - The number of samples. contiguous (bool, optional , default=False) - Whether to sample contiguous transitions. episodes (bool, optional , default=False) - Sample full episodes, instead of transitions. Return ExperienceReplay() containing the sampled transitions.","title":"sample"},{"location":"docs/cherry.models/","text":"cherry.models","title":"cherry.models"},{"location":"docs/cherry.models/#cherrymodels","text":"","title":"cherry.models"},{"location":"docs/cherry.nn.init/","text":"cherry.nn.init","title":"cherry.nn.init"},{"location":"docs/cherry.nn.init/#cherrynninit","text":"","title":"cherry.nn.init"},{"location":"docs/cherry.nn/","text":"cherry.nn","title":"cherry.nn"},{"location":"docs/cherry.nn/#cherrynn","text":"","title":"cherry.nn"},{"location":"docs/cherry.optim/","text":"cherry.optim Distributed Distributed(self, params=<required parameter>, opt=<required parameter>) Synchronizes optimizers across distributed replicas.","title":"cherry.optim"},{"location":"docs/cherry.optim/#cherryoptim","text":"","title":"cherry.optim"},{"location":"docs/cherry.optim/#distributed","text":"Distributed(self, params=<required parameter>, opt=<required parameter>) Synchronizes optimizers across distributed replicas.","title":"Distributed"},{"location":"docs/cherry.plot/","text":"exponential_smoothing exponential_smoothing(x, y=None, temperature=1.0) [Source] Decription Two side exponential moving average for smoothing a curve It performs regular exponential moving average twice from two different sides and then combines the results together. It performs better than one side exponential smoothing. Arguments x (ndarray/tensor/list) - x values, in accending order. y (ndarray/tensor/list) - y values. temperature (float, optional , default=1.0) - The higher, the smoother. Return ndarray - x values after resampling. ndarray - y values after smoothing. Credit Adapted from OpenAI's baselines implementation. Example from cherry.plot import exponential_smoothing x_smoothed, y_smoothed, _ = exponential_smoothing(x_original, y_original, temperature=3.)","title":"cherry.plot"},{"location":"docs/cherry.plot/#exponential_smoothing","text":"exponential_smoothing(x, y=None, temperature=1.0) [Source] Decription Two side exponential moving average for smoothing a curve It performs regular exponential moving average twice from two different sides and then combines the results together. It performs better than one side exponential smoothing. Arguments x (ndarray/tensor/list) - x values, in accending order. y (ndarray/tensor/list) - y values. temperature (float, optional , default=1.0) - The higher, the smoother. Return ndarray - x values after resampling. ndarray - y values after smoothing. Credit Adapted from OpenAI's baselines implementation. Example from cherry.plot import exponential_smoothing x_smoothed, y_smoothed, _ = exponential_smoothing(x_original, y_original, temperature=3.)","title":"exponential_smoothing"},{"location":"docs/cherry.rewards/","text":"cherry.rewards TODO: For both, assume torch tensors and vectorize. discount_rewards discount_rewards(gamma, rewards, dones, bootstrap=0.0) discount_rewards discount_rewards(gamma, rewards, dones, bootstrap=0.0) generalized_advantage_estimate generalized_advantage_estimate(gamma, tau, rewards, dones, values, next_value) generalized_advantage_estimate generalized_advantage_estimate(gamma, tau, rewards, dones, values, next_value)","title":"cherry.rewards"},{"location":"docs/cherry.rewards/#cherryrewards","text":"TODO: For both, assume torch tensors and vectorize.","title":"cherry.rewards"},{"location":"docs/cherry.rewards/#discount_rewards","text":"discount_rewards(gamma, rewards, dones, bootstrap=0.0)","title":"discount_rewards"},{"location":"docs/cherry.rewards/#discount_rewards_1","text":"discount_rewards(gamma, rewards, dones, bootstrap=0.0)","title":"discount_rewards"},{"location":"docs/cherry.rewards/#generalized_advantage_estimate","text":"generalized_advantage_estimate(gamma, tau, rewards, dones, values, next_value)","title":"generalized_advantage_estimate"},{"location":"docs/cherry.rewards/#generalized_advantage_estimate_1","text":"generalized_advantage_estimate(gamma, tau, rewards, dones, values, next_value)","title":"generalized_advantage_estimate"},{"location":"docs/cherry.utils/","text":"cherry.utils min_size min_size(tensor) Returns the minimium viewable size of a tensor. e.g. (1, 1, 3, 4) -> (3, 4) normalize normalize(tensor, epsilon=1e-08) Normalizes a tensor to zero mean and unit std.","title":"cherry.utils"},{"location":"docs/cherry.utils/#cherryutils","text":"","title":"cherry.utils"},{"location":"docs/cherry.utils/#min_size","text":"min_size(tensor) Returns the minimium viewable size of a tensor. e.g. (1, 1, 3, 4) -> (3, 4)","title":"min_size"},{"location":"docs/cherry.utils/#normalize","text":"normalize(tensor, epsilon=1e-08) Normalizes a tensor to zero mean and unit std.","title":"normalize"},{"location":"tutorials/continuous_ppo/","text":"","title":"Continuous ppo"},{"location":"tutorials/debug_rl/","text":"Debugging Reinforcement Learning Objectives: Show how cherry helps you catch bugs with defensive programming, Show how to use the VisdomLogger and the Record wrapper Provide some tips and tricks on debugging.","title":"Debugging Reinforcement Learning"},{"location":"tutorials/debug_rl/#debugging-reinforcement-learning","text":"Objectives: Show how cherry helps you catch bugs with defensive programming, Show how to use the VisdomLogger and the Record wrapper Provide some tips and tricks on debugging.","title":"Debugging Reinforcement Learning"},{"location":"tutorials/debugging_rl/","text":"Debugging Reinforcement Learning Objectives: Show how cherry helps you catch bugs with defensive programming, Show how to use the VisdomLogger and the Record wrapper Provide some tips and tricks on debugging.","title":"Debugging Reinforcement Learning"},{"location":"tutorials/debugging_rl/#debugging-reinforcement-learning","text":"Objectives: Show how cherry helps you catch bugs with defensive programming, Show how to use the VisdomLogger and the Record wrapper Provide some tips and tricks on debugging.","title":"Debugging Reinforcement Learning"},{"location":"tutorials/distributed_a2c/","text":"Tutorial on Distributed A2C","title":"Tutorial on Distributed A2C"},{"location":"tutorials/distributed_a2c/#tutorial-on-distributed-a2c","text":"","title":"Tutorial on Distributed A2C"},{"location":"tutorials/distributed_ppo/","text":"Distributed Training with PPO Objectives: Show how to use the control models, Show how to setup the distributed optimizer, Showcase the PPO methods. Pretty short too.","title":"Distributed Training with PPO"},{"location":"tutorials/distributed_ppo/#distributed-training-with-ppo","text":"Objectives: Show how to use the control models, Show how to setup the distributed optimizer, Showcase the PPO methods. Pretty short too.","title":"Distributed Training with PPO"},{"location":"tutorials/getting_started/","text":"Getting Started with Cherry Install Explain philosophy Overview of tools Use for a very simple examples: reinforce and q-learning on debug grid world.","title":"Getting Started with Cherry"},{"location":"tutorials/getting_started/#getting-started-with-cherry","text":"Install Explain philosophy Overview of tools Use for a very simple examples: reinforce and q-learning on debug grid world.","title":"Getting Started with Cherry"},{"location":"tutorials/recurrent_a2c/","text":"Recurrent Policy Gradients with A2C Objectives: Show how to use Atari models + wrappers. Show how to implement recurrent policy gradients with cherry. That's it, very short.","title":"Recurrent Policy Gradients with A2C"},{"location":"tutorials/recurrent_a2c/#recurrent-policy-gradients-with-a2c","text":"Objectives: Show how to use Atari models + wrappers. Show how to implement recurrent policy gradients with cherry. That's it, very short.","title":"Recurrent Policy Gradients with A2C"}]}